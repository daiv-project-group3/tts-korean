{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8_jNGQZF3lke"
   },
   "source": [
    "# Face DCGAN 튜토리얼\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aw0H1hHr3lki",
    "outputId": "05e45bce-ba07-402d-ed98-22e26224e547",
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:02.784182Z",
     "start_time": "2024-11-19T15:54:02.778330Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchaudio"
   ],
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:03.627002Z",
     "start_time": "2024-11-19T15:54:03.624089Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:03.636517Z",
     "start_time": "2024-11-19T15:54:03.628011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, negative_slope=0.01):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.skip_connection = nn.Conv1d(in_channels, out_channels, kernel_size=1, stride=1, padding=0) if in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.skip_connection(x)\n",
    "        x = self.leaky_relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        return self.leaky_relu(x + residual)"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tLU8xZsp3lkj",
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:03.652584Z",
     "start_time": "2024-11-19T15:54:03.638536Z"
    }
   },
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(80, 512, kernel_size=7, stride=1, padding=3)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # 첫 번째 업샘플링 레이어 (stride=4, kernel_size=16)\n",
    "        self.deconv1 = nn.ConvTranspose1d(512, 512, kernel_size=10, stride=2, padding=4)\n",
    "\n",
    "        # 두 번째 업샘플링 레이어 (stride=4, kernel_size=16)\n",
    "        self.deconv2 = nn.ConvTranspose1d(512, 1, kernel_size=10, stride=2, padding=4)\n",
    "\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.deconv1(x))\n",
    "        x = self.deconv2(x)\n",
    "        return self.tanh(x)"
   ],
   "outputs": [],
   "execution_count": 72
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HKcXZsZ63-Rh",
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:03.665549Z",
     "start_time": "2024-11-19T15:54:03.654594Z"
    }
   },
   "source": [
    "class MultiScaleDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiScaleDiscriminator, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv1d(4, 16, kernel_size=15, stride=1, padding=7),  # 첫 번째 레이어의 입력 채널을 4로 설정\n",
    "            nn.Conv1d(16, 32, kernel_size=15, stride=2, padding=7),\n",
    "            nn.Conv1d(32, 64, kernel_size=15, stride=2, padding=7),\n",
    "            nn.Conv1d(64, 128, kernel_size=15, stride=2, padding=7),\n",
    "        ])\n",
    "        self.final_conv = nn.Conv1d(128, 1, 3, 1, 1)\n",
    "\n",
    "        # real_audio 텐서를 4개의 채널로 변환하는 Conv1d 추가\n",
    "        self.channel_conv = nn.Conv1d(4, 4, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # real_audio의 채널을 4로 변환\n",
    "        x = self.channel_conv(x)\n",
    "        features = []\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            features.append(x)\n",
    "        x = self.final_conv(x)\n",
    "        return x, features"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r4SG_juz3lkk",
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:03.677114Z",
     "start_time": "2024-11-19T15:54:03.669556Z"
    }
   },
   "source": [
    "class MultiPeriodDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiPeriodDiscriminator, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Conv1d(4, 16, kernel_size=15, stride=1, padding=7),  # 첫 번째 레이어\n",
    "            nn.Conv1d(16, 32, kernel_size=16, stride=2, padding=8),  # 주기적 특징을 잡기 위해 kernel_size, stride 조정\n",
    "            nn.Conv1d(32, 64, kernel_size=32, stride=4, padding=16),  # 주기적인 정보 캡처\n",
    "            nn.Conv1d(64, 128, kernel_size=64, stride=8, padding=32),  # 더 큰 주기 정보 캡처\n",
    "        ])\n",
    "        self.final_conv = nn.Conv1d(128, 1, 3, 1, 1)\n",
    "        \n",
    "        self.channel_conv = nn.Conv1d(4, 4, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.channel_conv(x)\n",
    "        features = []\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            features.append(x)\n",
    "        x = self.final_conv(x)\n",
    "        return x, features"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:03.839151Z",
     "start_time": "2024-11-19T15:54:03.834448Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:04.014628Z",
     "start_time": "2024-11-19T15:54:04.007418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def l1_loss(x, y):\n",
    "    return torch.mean(torch.abs(x - y))"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:04.171159Z",
     "start_time": "2024-11-19T15:54:04.158873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HiFiGANLoss(nn.Module):\n",
    "    def __init__(self, device='cuda'):\n",
    "        super(HiFiGANLoss, self).__init__()\n",
    "        self.device = device\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        \n",
    "        # Mel-spectrogram 계산을 위한 설정\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=22050, n_fft=1024, hop_length=256, n_mels=80, normalized=True\n",
    "        ).to(device)\n",
    "\n",
    "    def mel_spectrogram_loss(self, real_wave, fake_wave):\n",
    "        \"\"\" Mel-spectrogram 손실 (L1 Loss 또는 L2 Loss) \"\"\"\n",
    "        # Mel-spectrogram 계산\n",
    "        real_mel = self.mel_transform(real_wave)\n",
    "        fake_mel = self.mel_transform(fake_wave)\n",
    "        print(real_mel.size())\n",
    "        \n",
    "        # Mel-spectrogram 간의 L1 손실 계산\n",
    "        mel_loss = self.l1_loss(fake_mel, real_mel)\n",
    "        return mel_loss\n",
    "\n",
    "    def feature_loss(self, real_features, fake_features):\n",
    "        \"\"\" 특징 손실 (L1 Loss) \"\"\"\n",
    "        return self.l1_loss(fake_features, real_features)\n",
    "\n",
    "    def generator_loss(self, fake_output):\n",
    "        \"\"\" 제너레이터의 손실 함수 (LSGAN 손실) \"\"\"\n",
    "        return torch.mean((fake_output - 1) ** 2)\n",
    "\n",
    "    def discriminator_loss(self, real_output, fake_output):\n",
    "        \"\"\" 디스크리미네이터의 손실 함수 (LSGAN 손실) \"\"\"\n",
    "        real_loss = torch.mean((real_output - 1) ** 2)\n",
    "        fake_loss = torch.mean(fake_output ** 2)\n",
    "        return (real_loss + fake_loss) * 0.5\n",
    "\n",
    "    def forward(self, real_wave, fake_wave, real_features, fake_features):\n",
    "        \"\"\" 전체 손실 계산 \"\"\"\n",
    "        # 1. Generator Loss (fake_output은 디스크리미네이터의 출력)\n",
    "        fake_output = fake_wave.detach()  # Generator가 생성한 오디오\n",
    "        generator_loss = self.generator_loss(fake_output)\n",
    "\n",
    "        # 2. Discriminator Loss\n",
    "        real_output = real_wave  # 실제 오디오\n",
    "        discriminator_loss = self.discriminator_loss(real_output, fake_output)\n",
    "\n",
    "        # 3. Mel-spectrogram Loss\n",
    "        mel_loss = self.mel_spectrogram_loss(real_wave, fake_wave)\n",
    "\n",
    "        # 4. Feature Loss\n",
    "        feature_loss = self.feature_loss(real_features, fake_features)\n",
    "\n",
    "        # 종합적인 손실 (가중치를 조정할 수 있음)\n",
    "        total_loss = generator_loss + discriminator_loss + mel_loss + feature_loss\n",
    "        return total_loss, generator_loss, discriminator_loss, mel_loss, feature_loss\n"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0_zazOsx3lkl",
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:04.317706Z",
     "start_time": "2024-11-19T15:54:04.308622Z"
    }
   },
   "source": [
    "\n",
    "class MRFDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MRFDiscriminator, self).__init__()\n",
    "        # 채널 수를 맞추기 위한 Conv1d 레이어 추가\n",
    "        self.channel_adjustment = nn.Conv1d(128, 512, kernel_size=3, padding=1)\n",
    "        \n",
    "        # ResBlock 추가\n",
    "        self.resblock1 = ResBlock(512, 256)\n",
    "        self.resblock2 = ResBlock(256, 128)\n",
    "        \n",
    "        # Fusion layer\n",
    "        self.fusion_layer = nn.Conv1d(128, 64, kernel_size=3, padding=1)\n",
    "        self.output_layer = nn.Conv1d(64, 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, msd_features, mpd_features):\n",
    "        # MSD와 MPD에서 나온 특징을 결합\n",
    "        fused_features = torch.cat([msd_features[-1], mpd_features[-1]], dim=1)  # 채널 수가 맞는지 확인\n",
    "        \n",
    "        # 채널 수를 맞추기 위해 adjustment\n",
    "        fused_features = self.channel_adjustment(fused_features)\n",
    "        \n",
    "        # ResBlock을 통한 특성 추출\n",
    "        x = self.resblock1(fused_features)\n",
    "        x = self.resblock2(x)\n",
    "        \n",
    "        # Fusion layer\n",
    "        x = self.fusion_layer(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ],
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:04.511647Z",
     "start_time": "2024-11-19T15:54:04.503485Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RandomMelDataset(Dataset):\n",
    "    def __init__(self, num_samples, mel_spec_len=80, audio_len=800):\n",
    "        self.num_samples = num_samples\n",
    "        self.mel_spec_len = mel_spec_len\n",
    "        self.audio_len = audio_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 랜덤 Mel-spectrogram과 오디오 (여기선 예시로 랜덤 데이터 사용)\n",
    "        mel_spec = torch.randn(self.mel_spec_len, 200)  # Mel spectrogram (4, 80, 200)\n",
    "        audio = torch.randn(self.audio_len)  # 오디오 시퀀스 (1, 800)\n",
    "        return mel_spec, audio"
   ],
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:04.809695Z",
     "start_time": "2024-11-19T15:54:04.803434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 데이터셋과 DataLoader\n",
    "dataset = RandomMelDataset(num_samples=1000)  # 1000개의 샘플 사용\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)  # 배치 크기 4로 설정"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcEOXNWC3lkl"
   },
   "source": ""
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2cM2obc53lkm",
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:05.151222Z",
     "start_time": "2024-11-19T15:54:05.085063Z"
    }
   },
   "source": [
    "generator = Generator().cuda()\n",
    "msd = MultiScaleDiscriminator().cuda()\n",
    "mpd = MultiPeriodDiscriminator().cuda()\n",
    "mrf_discriminator = MRFDiscriminator().cuda()  # MRF discriminator 추가\n",
    "\n",
    "optimizer_g = optim.Adam(generator.parameters(), lr=0.0001, betas=(0.5, 0.9))\n",
    "optimizer_d = optim.Adam(list(msd.parameters()) + list(mpd.parameters()) + list(mrf_discriminator.parameters()), lr=0.0001, betas=(0.5, 0.9))"
   ],
   "outputs": [],
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tM9Z_Xzu3lkn",
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:05.307176Z",
     "start_time": "2024-11-19T15:54:05.300823Z"
    }
   },
   "source": "num_epochs = 100",
   "outputs": [],
   "execution_count": 81
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hQyu9tST3lkn",
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:05.489067Z",
     "start_time": "2024-11-19T15:54:05.479863Z"
    }
   },
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 82
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Zu-8_Rep3lko",
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:05.702486Z",
     "start_time": "2024-11-19T15:54:05.637307Z"
    }
   },
   "source": [
    "generator = Generator().to(device)\n",
    "msd = MultiScaleDiscriminator().to(device)\n",
    "mpd = MultiPeriodDiscriminator().to(device)\n",
    "\n",
    "opt_g = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.8, 0.99))\n",
    "opt_d = optim.Adam(list(msd.parameters()) + list(mpd.parameters()), lr=0.0002, betas=(0.8, 0.99))\n",
    "\n",
    "mel_spec = torch.randn(4, 80, 1000).to(device)\n",
    "real_audio = torch.randn(4, 1, 16000).to(device)\n",
    "hifi_gan_loss = HiFiGANLoss(device=device)"
   ],
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:05.734411Z",
     "start_time": "2024-11-19T15:54:05.726506Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fake_audio = generator(mel_spec)\n",
    "print(fake_audio.size())\n",
    "print(real_audio.size())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 4000])\n",
      "torch.Size([4, 1, 16000])\n"
     ]
    }
   ],
   "execution_count": 84
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IV3edow93lko",
    "ExecuteTime": {
     "end_time": "2024-11-19T15:54:11.544187Z",
     "start_time": "2024-11-19T15:54:06.055529Z"
    }
   },
   "source": [
    "for epoch in range(num_epochs):\n",
    "    generator.train()\n",
    "    msd.train()\n",
    "    mpd.train()\n",
    "    mrf_discriminator.train()\n",
    "\n",
    "    total_g_loss = 0\n",
    "    total_d_loss = 0\n",
    "    total_mel_loss = 0\n",
    "    total_feature_loss = 0\n",
    "\n",
    "    for batch_idx, (mel_spec, real_audio) in enumerate(train_loader):\n",
    "        mel_spec, real_audio = mel_spec.cuda(), real_audio.cuda()\n",
    "\n",
    "        fake_audio = generator(mel_spec)\n",
    "        fake_audio = torch.squeeze(fake_audio,dim = 1)\n",
    "\n",
    "        real_scores_msd, real_features_msd = msd(real_audio)\n",
    "        fake_scores_msd, fake_features_msd = msd(fake_audio.detach())\n",
    "        print(len(real_features_msd))\n",
    "\n",
    "        real_scores_mpd, real_features_mpd = mpd(real_audio)\n",
    "        fake_scores_mpd, fake_features_mpd = mpd(fake_audio.detach())\n",
    "\n",
    "        # MRF 결합된 판별자 사용\n",
    "        fake_mrf_scores = mrf_discriminator(fake_features_msd, fake_features_mpd)\n",
    "        real_mrf_scores = mrf_discriminator(real_features_msd, real_features_mpd)\n",
    "\n",
    "        # Generator Loss\n",
    "        g_loss_msd = l1_loss(fake_scores_msd, real_scores_msd)\n",
    "        g_loss_mpd = l1_loss(fake_scores_mpd, real_scores_mpd)\n",
    "        g_loss_mrf = l1_loss(fake_mrf_scores, torch.ones_like(fake_mrf_scores))\n",
    "        generator_loss = g_loss_msd + g_loss_mpd + g_loss_mrf\n",
    "\n",
    "        # Discriminator Loss\n",
    "        d_loss_msd_real = l1_loss(real_scores_msd, torch.ones_like(real_scores_msd))\n",
    "        d_loss_msd_fake = l1_loss(fake_scores_msd, torch.zeros_like(fake_scores_msd))\n",
    "        d_loss_msd = (d_loss_msd_real + d_loss_msd_fake) * 0.5\n",
    "\n",
    "        d_loss_mpd_real = l1_loss(real_scores_mpd, torch.ones_like(real_scores_mpd))\n",
    "        d_loss_mpd_fake = l1_loss(fake_scores_mpd, torch.zeros_like(fake_scores_mpd))\n",
    "        d_loss_mpd = (d_loss_mpd_real + d_loss_mpd_fake) * 0.5\n",
    "\n",
    "        d_loss_mrf_real = l1_loss(real_mrf_scores, torch.ones_like(real_mrf_scores))\n",
    "        d_loss_mrf_fake = l1_loss(fake_mrf_scores, torch.zeros_like(fake_mrf_scores))\n",
    "        d_loss_mrf = (d_loss_mrf_real + d_loss_mrf_fake) * 0.5\n",
    "\n",
    "        discriminator_loss = d_loss_msd + d_loss_mpd + d_loss_mrf\n",
    "\n",
    "        # Mel-spectrogram Loss 추가\n",
    "        mel_loss = hifi_gan_loss.mel_spectrogram_loss(real_audio, fake_audio)\n",
    "\n",
    "        # Feature Loss 추가\n",
    "\n",
    "        # 최적화\n",
    "        optimizer_g.zero_grad()\n",
    "        (generator_loss + mel_loss).backward(retain_graph=True)  # Mel-spectrogram loss, Feature loss 포함\n",
    "        optimizer_g.step()\n",
    "\n",
    "        optimizer_d.zero_grad()\n",
    "        discriminator_loss.backward()\n",
    "        optimizer_d.step()\n",
    "\n",
    "        total_g_loss += generator_loss.item() + mel_loss.item()\n",
    "        total_d_loss += discriminator_loss.item()\n",
    "        total_mel_loss += mel_loss.item()\n",
    "\n",
    "    # 에폭마다 출력\n",
    "    print(f\"Epoch [{epoch}/{num_epochs}], Generator Loss: {total_g_loss / len(train_loader):.4f}, Discriminator Loss: {total_d_loss / len(train_loader):.4f}, Mel Loss: {total_mel_loss / len(train_loader):.4f}, Feature Loss: {total_feature_loss / len(train_loader):.4f}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n",
      "4\n",
      "torch.Size([4, 80, 4])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[85], line 57\u001B[0m\n\u001B[0;32m     53\u001B[0m \u001B[38;5;66;03m# Feature Loss 추가\u001B[39;00m\n\u001B[0;32m     54\u001B[0m \n\u001B[0;32m     55\u001B[0m \u001B[38;5;66;03m# 최적화\u001B[39;00m\n\u001B[0;32m     56\u001B[0m optimizer_g\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m---> 57\u001B[0m \u001B[43m(\u001B[49m\u001B[43mgenerator_loss\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mmel_loss\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Mel-spectrogram loss, Feature loss 포함\u001B[39;00m\n\u001B[0;32m     58\u001B[0m optimizer_g\u001B[38;5;241m.\u001B[39mstep()\n\u001B[0;32m     60\u001B[0m optimizer_d\u001B[38;5;241m.\u001B[39mzero_grad()\n",
      "File \u001B[1;32m~\\PycharmProjects\\CLIP\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001B[0m, in \u001B[0;36mTensor.backward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    571\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    572\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[0;32m    573\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[0;32m    574\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    579\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs,\n\u001B[0;32m    580\u001B[0m     )\n\u001B[1;32m--> 581\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\n\u001B[0;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\CLIP\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[0m, in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    342\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[0;32m    344\u001B[0m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[0;32m    345\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[0;32m    346\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[1;32m--> 347\u001B[0m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    348\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    349\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    350\u001B[0m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    351\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    352\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    353\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    354\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    355\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\CLIP\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001B[0m, in \u001B[0;36m_engine_run_backward\u001B[1;34m(t_outputs, *args, **kwargs)\u001B[0m\n\u001B[0;32m    823\u001B[0m     unregister_hooks \u001B[38;5;241m=\u001B[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[0;32m    824\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 825\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[0;32m    826\u001B[0m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[0;32m    827\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[0;32m    828\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    829\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T15:11:01.612489Z",
     "start_time": "2024-11-19T15:11:01.611355Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T13:54:56.316452Z",
     "start_time": "2024-11-19T13:54:56.313499Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": 188
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
