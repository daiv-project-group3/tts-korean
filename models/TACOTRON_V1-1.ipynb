{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:59:41.779823Z",
     "start_time": "2024-11-19T14:59:41.764694Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import librosa\n",
    "from jamo import hangul_to_jamo\n",
    "import librosa.feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "855f1d8671786fc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:40:45.878273Z",
     "start_time": "2024-11-19T14:40:45.874416Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b1ec414a8722034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:40:46.032011Z",
     "start_time": "2024-11-19T14:40:46.029093Z"
    }
   },
   "outputs": [],
   "source": [
    "def text_to_sequence(text):\n",
    "    # 한글 자모 분해\n",
    "    sequence = []\n",
    "    for char in text:\n",
    "        if '가' <= char <= '힣':\n",
    "            jamos = list(hangul_to_jamo(char))\n",
    "            sequence.extend(jamos)\n",
    "        else:\n",
    "            sequence.append(char)\n",
    "    return sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c97f2d7f2cde741",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:40:46.193609Z",
     "start_time": "2024-11-19T14:40:46.172961Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '여기에 파일'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m all_texts \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m metadata_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m여기에 파일\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmetadata_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m         parts \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    322\u001b[0m     )\n\u001b[1;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '여기에 파일'"
     ]
    }
   ],
   "source": [
    "# 예시: 데이터셋의 모든 텍스트를 수집\n",
    "all_texts = []\n",
    "metadata_path = '여기에 파일'\n",
    "\n",
    "with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('|')\n",
    "        if len(parts) >= 2:\n",
    "            path, text = parts[0], parts[1]\n",
    "            all_texts.append(text)\n",
    "\n",
    "# 모든 텍스트에서 고유한 토큰 추출\n",
    "unique_tokens = set()\n",
    "for text in all_texts:\n",
    "    sequence = text_to_sequence(text)\n",
    "    unique_tokens.update(sequence)\n",
    "\n",
    "# 토큰과 인덱스 매핑\n",
    "vocab = {token: idx for idx, token in enumerate(sorted(unique_tokens))}\n",
    "vocab['<pad>'] = len(vocab)\n",
    "vocab['<unk>'] = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7aca187df671f0fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:41:13.753713Z",
     "start_time": "2024-11-19T14:41:13.749318Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_mask_from_lengths(lengths, max_len=None):\n",
    "    batch_size = lengths.size(0)\n",
    "    if max_len is None:\n",
    "        max_len = torch.max(lengths).item()\n",
    "    ids = torch.arange(0, max_len, device=lengths.device).unsqueeze(0).expand(batch_size, -1)\n",
    "    mask = ids < lengths.unsqueeze(1)\n",
    "    return mask  # (batch_size, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22f2db585f32c26f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:40:46.534224Z",
     "start_time": "2024-11-19T14:40:46.529448Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_wav(path, sr=22050):\n",
    "    wav, _ = librosa.load(path, sr=sr)\n",
    "    return wav\n",
    "\n",
    "def wav_to_mel(wav, sr=22050, n_fft=1024, hop_length=256, win_length=1024, n_mels=80, fmin=0, fmax=8000):\n",
    "    # STFT\n",
    "    stft = librosa.stft(wav, n_fft=n_fft, hop_length=hop_length, win_length=win_length)\n",
    "    # 멜 스펙트로그램\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(S=np.abs(stft)**2, sr=sr, n_fft=n_fft, hop_length=hop_length, n_mels=n_mels, fmin=fmin, fmax=fmax)\n",
    "    # 로그 스케일\n",
    "    mel_spectrogram = np.log(np.clip(mel_spectrogram, a_min=1e-5, a_max=None))\n",
    "    return mel_spectrogram\n",
    "\n",
    "def normalize_mel(mel_spectrogram):\n",
    "    # 정규화: [-4, 4] -> [0, 1]\n",
    "    mel_spectrogram = np.clip(mel_spectrogram, a_min=-4, a_max=4)\n",
    "    mel_spectrogram = (mel_spectrogram + 4) / 8\n",
    "    return mel_spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc2ac5f7e61854a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:40:46.955501Z",
     "start_time": "2024-11-19T14:40:46.950140Z"
    }
   },
   "outputs": [],
   "source": [
    "class TacotronDataset(Dataset):\n",
    "    def __init__(self, metadata_path, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.data = self.load_metadata(metadata_path)\n",
    "\n",
    "    def load_metadata(self, metadata_path):\n",
    "        data = []\n",
    "        with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split('|')\n",
    "                if len(parts) >= 2:\n",
    "                    path, text = parts[0], parts[1]\n",
    "                    data.append((path, text))\n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, text = self.data[idx]\n",
    "        # 텍스트 처리\n",
    "        sequence = text_to_sequence(text)\n",
    "        sequence = [self.vocab.get(char, self.vocab['<unk>']) for char in sequence]\n",
    "        sequence = torch.LongTensor(sequence)\n",
    "        input_length = sequence.size(0)\n",
    "        # 오디오 처리\n",
    "        wav = load_wav(path)\n",
    "        mel = wav_to_mel(wav)\n",
    "        mel = normalize_mel(mel)\n",
    "        mel = torch.FloatTensor(mel)\n",
    "        mel = mel.transpose(0, 1)  # (시간 축, 멜 밴드 수)\n",
    "        mel_length = mel.size(0)\n",
    "        return sequence, input_length, mel, mel_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5870ef28affefddd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:40:47.319162Z",
     "start_time": "2024-11-19T14:40:47.313078Z"
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    sequences, input_lengths, mels, mel_lengths = zip(*batch)\n",
    "    # 입력 시퀀스 패딩\n",
    "    max_input_len = max(input_lengths)\n",
    "    padded_sequences = torch.zeros(len(sequences), max_input_len, dtype=torch.long)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        padded_sequences[i, :seq.size(0)] = seq\n",
    "    # 멜 스펙트로그램 패딩\n",
    "    max_mel_len = max(mel_lengths)\n",
    "    num_mels = mels[0].size(1)\n",
    "    padded_mels = torch.zeros(len(mels), max_mel_len, num_mels)\n",
    "    for i, mel in enumerate(mels):\n",
    "        padded_mels[i, :mel.size(0), :] = mel\n",
    "    # 길이 텐서로 변환\n",
    "    input_lengths = torch.LongTensor(input_lengths)\n",
    "    mel_lengths = torch.LongTensor(mel_lengths)\n",
    "    return padded_sequences, input_lengths, padded_mels, mel_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f78bb82c602cc293",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:40:47.863977Z",
     "start_time": "2024-11-19T14:40:47.859574Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        conv_channels = 512\n",
    "        self.prenet = nn.Sequential(\n",
    "            nn.Conv1d(embedding_dim, conv_channels, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(conv_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv1d(conv_channels, conv_channels, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(conv_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Conv1d(conv_channels, conv_channels, kernel_size=5, padding=2),\n",
    "            nn.BatchNorm1d(conv_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(conv_channels, 512, num_layers=1, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, seq_len, embedding_dim)\n",
    "        x = x.transpose(1, 2)  # (batch_size, embedding_dim, seq_len)\n",
    "        x = self.prenet(x)     # (batch_size, conv_channels, seq_len)\n",
    "        x = x.transpose(1, 2)  # (batch_size, seq_len, conv_channels)\n",
    "        outputs, _ = self.lstm(x)\n",
    "        return outputs  # (batch_size, seq_len, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c8a60b65f07c9cd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:40:48.320352Z",
     "start_time": "2024-11-19T14:40:48.314308Z"
    }
   },
   "outputs": [],
   "source": [
    "class PreNet(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(PreNet, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1328724f649cf03b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:40:48.881688Z",
     "start_time": "2024-11-19T14:40:48.877362Z"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, attention_rnn_dim, encoder_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.query_layer = nn.Linear(attention_rnn_dim, 128)\n",
    "        self.memory_layer = nn.Linear(encoder_dim, 128)\n",
    "        self.v = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, attention_hidden_state, memory, mask):\n",
    "        # attention_hidden_state: (batch_size, attention_rnn_dim)\n",
    "        # memory: (batch_size, seq_len, encoder_dim)\n",
    "        # mask: (batch_size, seq_len)\n",
    "        processed_query = self.query_layer(attention_hidden_state).unsqueeze(1)  # (batch_size, 1, 128)\n",
    "        processed_memory = self.memory_layer(memory)  # (batch_size, seq_len, 128)\n",
    "        energies = self.v(torch.tanh(processed_query + processed_memory)).squeeze(-1)  # (batch_size, seq_len)\n",
    "        energies.data.masked_fill_(mask == 0, -float('inf'))\n",
    "        attention_weights = F.softmax(energies, dim=1)  # (batch_size, seq_len)\n",
    "        attention_context = torch.bmm(attention_weights.unsqueeze(1), memory).squeeze(1)  # (batch_size, encoder_dim)\n",
    "        return attention_context, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "449fe5e9ab76f673",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:40:49.553332Z",
     "start_time": "2024-11-19T14:40:49.545715Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.prenet = PreNet(80)\n",
    "        self.attention_rnn = nn.LSTMCell(256 + 512, 1024)\n",
    "        self.attention_layer = Attention(1024, 512)\n",
    "        self.decoder_rnn = nn.LSTMCell(1024 + 512, 1024)\n",
    "        self.linear_projection = nn.Linear(1024 + 512, 80)\n",
    "        self.stop_projection = nn.Linear(1024 + 512, 1)\n",
    "\n",
    "    def forward(self, memory, decoder_inputs, memory_lengths):\n",
    "        # memory: (batch_size, seq_len, 512)\n",
    "        # decoder_inputs: (batch_size, max_time, 80)\n",
    "        # memory_lengths: (batch_size)\n",
    "        batch_size = memory.size(0)\n",
    "        max_time = decoder_inputs.size(1)\n",
    "        device = memory.device\n",
    "\n",
    "        # 마스크 생성\n",
    "        mask = ~get_mask_from_lengths(memory_lengths, max_len=memory.size(1)).to(device)\n",
    "\n",
    "        # 초기화\n",
    "        attention_hidden = torch.zeros(batch_size, 1024).to(device)\n",
    "        attention_cell = torch.zeros(batch_size, 1024).to(device)\n",
    "        decoder_hidden = torch.zeros(batch_size, 1024).to(device)\n",
    "        decoder_cell = torch.zeros(batch_size, 1024).to(device)\n",
    "        attention_context = torch.zeros(batch_size, 512).to(device)\n",
    "        mel_outputs = []\n",
    "        stop_outputs = []\n",
    "\n",
    "        for t in range(max_time):\n",
    "            prenet_input = decoder_inputs[:, t, :]  # (batch_size, 80)\n",
    "            prenet_output = self.prenet(prenet_input)\n",
    "            # 어텐션 RNN\n",
    "            attention_rnn_input = torch.cat((prenet_output, attention_context), dim=-1)\n",
    "            attention_hidden, attention_cell = self.attention_rnn(attention_rnn_input, (attention_hidden, attention_cell))\n",
    "            # 어텐션 메커니즘\n",
    "            attention_context, attention_weights = self.attention_layer(attention_hidden, memory, mask)\n",
    "            # 디코더 RNN\n",
    "            decoder_rnn_input = torch.cat((attention_hidden, attention_context), dim=-1)\n",
    "            decoder_hidden, decoder_cell = self.decoder_rnn(decoder_rnn_input, (decoder_hidden, decoder_cell))\n",
    "            # 출력 계산\n",
    "            decoder_output = torch.cat((decoder_hidden, attention_context), dim=-1)\n",
    "            mel_output = self.linear_projection(decoder_output)\n",
    "            stop_output = self.stop_projection(decoder_output)\n",
    "            mel_outputs.append(mel_output.unsqueeze(1))\n",
    "            stop_outputs.append(stop_output.unsqueeze(1))\n",
    "\n",
    "        mel_outputs = torch.cat(mel_outputs, dim=1)  # (batch_size, max_time, 80)\n",
    "        stop_outputs = torch.cat(stop_outputs, dim=1)  # (batch_size, max_time, 1)\n",
    "        return mel_outputs, stop_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2cc29992539b37a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:40:50.378355Z",
     "start_time": "2024-11-19T14:40:50.373723Z"
    }
   },
   "outputs": [],
   "source": [
    "class PostNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PostNet, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        in_channels = [80] + [512] * 4\n",
    "        out_channels = [512] * 4 + [80]\n",
    "        for i in range(5):\n",
    "            self.layers.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(in_channels[i], out_channels[i], kernel_size=5, padding=2),\n",
    "                    nn.BatchNorm1d(out_channels[i]),\n",
    "                    nn.Tanh() if i < 4 else nn.Identity(),\n",
    "                    nn.Dropout(0.5)\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # (batch_size, 80, max_time)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        x = x.transpose(1, 2)  # (batch_size, max_time, 80)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c89e1df6a4a35611",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:40:51.081988Z",
     "start_time": "2024-11-19T14:40:51.077644Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tacotron2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=512):\n",
    "        super(Tacotron2, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.encoder = Encoder(embedding_dim)\n",
    "        self.decoder = Decoder()\n",
    "        self.postnet = PostNet()\n",
    "\n",
    "    def forward(self, text_inputs, mel_targets, input_lengths):\n",
    "        # 인코더\n",
    "        embedded_inputs = self.embedding(text_inputs)  # (batch_size, seq_len, embedding_dim)\n",
    "        encoder_outputs = self.encoder(embedded_inputs)\n",
    "\n",
    "        # 디코더 입력 준비\n",
    "        decoder_inputs = self._get_decoder_inputs(mel_targets)\n",
    "\n",
    "        # 디코더\n",
    "        mel_outputs, stop_outputs = self.decoder(encoder_outputs, decoder_inputs, input_lengths)\n",
    "\n",
    "        # 포스트넷\n",
    "        mel_outputs_postnet = mel_outputs + self.postnet(mel_outputs)\n",
    "\n",
    "        return mel_outputs, mel_outputs_postnet, stop_outputs\n",
    "\n",
    "    def _get_decoder_inputs(self, mel_targets):\n",
    "        go_frame = torch.zeros(mel_targets.size(0), 1, mel_targets.size(2)).to(mel_targets.device)\n",
    "        decoder_inputs = torch.cat((go_frame, mel_targets[:, :-1, :]), dim=1)\n",
    "        return decoder_inputs  # (batch_size, mel_length, num_mels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f77f6c5da3e4171b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:41:30.396305Z",
     "start_time": "2024-11-19T14:41:30.392103Z"
    }
   },
   "outputs": [],
   "source": [
    "def Tacotron2Loss(mel_outputs, mel_outputs_postnet, mel_targets, stop_outputs, stop_targets, mel_mask):\n",
    "    # 마스킹 적용하여 손실 계산\n",
    "    mel_loss = F.mse_loss(mel_outputs[mel_mask], mel_targets[mel_mask])\n",
    "    mel_postnet_loss = F.mse_loss(mel_outputs_postnet[mel_mask], mel_targets[mel_mask])\n",
    "    stop_token_loss = F.binary_cross_entropy_with_logits(stop_outputs.squeeze(-1), stop_targets, reduction='none')\n",
    "    stop_token_loss = (stop_token_loss * mel_mask.squeeze(-1)).sum() / mel_mask.sum()\n",
    "\n",
    "    # 총 손실\n",
    "    total_loss = mel_loss + mel_postnet_loss + 0.1 * stop_token_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "84198ca2ce47ba0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:41:36.449112Z",
     "start_time": "2024-11-19T14:41:36.436157Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m Tacotron2(vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[43mvocab\u001b[49m))\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, betas\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0.9\u001b[39m, \u001b[38;5;241m0.999\u001b[39m), eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "model = Tacotron2(vocab_size=len(vocab)).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c872ba342192d2e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:42:37.093611Z",
     "start_time": "2024-11-19T14:42:37.076471Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 데이터로더 생성\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m TacotronDataset(metadata_path, \u001b[43mvocab\u001b[49m)\n\u001b[0;32m      3\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, collate_fn\u001b[38;5;241m=\u001b[39mcollate_fn)\n\u001b[0;32m      5\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab' is not defined"
     ]
    }
   ],
   "source": [
    "# 데이터로더 생성\n",
    "dataset = TacotronDataset(metadata_path, vocab)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "num_epochs = 100\n",
    "log_interval = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (text_inputs, input_lengths, mel_targets, mel_lengths) in enumerate(data_loader):\n",
    "        text_inputs = text_inputs.to(device)\n",
    "        input_lengths = input_lengths.to(device)\n",
    "        mel_targets = mel_targets.to(device)\n",
    "        mel_lengths = mel_lengths.to(device)\n",
    "\n",
    "        # 마스크 생성\n",
    "        mel_mask = get_mask_from_lengths(mel_lengths, max_len=mel_targets.size(1)).unsqueeze(-1).to(device)\n",
    "\n",
    "        # 스톱 토큰 타겟 생성\n",
    "        stop_targets = torch.zeros(mel_targets.size(0), mel_targets.size(1)).to(device)\n",
    "        for j, length in enumerate(mel_lengths):\n",
    "            stop_targets[j, length - 1:] = 1.0\n",
    "\n",
    "        # 모델 출력\n",
    "        mel_outputs, mel_outputs_postnet, stop_outputs = model(text_inputs, mel_targets, input_lengths)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = Tacotron2Loss(mel_outputs, mel_outputs_postnet, mel_targets, stop_outputs, stop_targets, mel_mask)\n",
    "\n",
    "        # 역전파 및 옵티마이저 스텝\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        # 로그 출력\n",
    "        if i % log_interval == 0:\n",
    "            print(f'Epoch: {epoch}, Step: {i}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "abe91be104860f46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:42:43.945199Z",
     "start_time": "2024-11-19T14:42:43.938659Z"
    }
   },
   "outputs": [],
   "source": [
    "def synthesize(model, text, vocab, max_decoder_steps=1000):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # 텍스트 전처리\n",
    "        sequence = text_to_sequence(text)\n",
    "        sequence = [vocab.get(char, vocab['<unk>']) for char in sequence]\n",
    "        text_inputs = torch.LongTensor(sequence).unsqueeze(0).to(device)\n",
    "        input_lengths = torch.LongTensor([len(sequence)]).to(device)\n",
    "\n",
    "        # 인코더\n",
    "        embedded_inputs = model.embedding(text_inputs)\n",
    "        encoder_outputs = model.encoder(embedded_inputs)\n",
    "\n",
    "        # 디코더 초기화\n",
    "        batch_size = 1\n",
    "        attention_hidden = torch.zeros(batch_size, 1024).to(device)\n",
    "        attention_cell = torch.zeros(batch_size, 1024).to(device)\n",
    "        decoder_hidden = torch.zeros(batch_size, 1024).to(device)\n",
    "        decoder_cell = torch.zeros(batch_size, 1024).to(device)\n",
    "        attention_context = torch.zeros(batch_size, 512).to(device)\n",
    "        mel_outputs = []\n",
    "        stop_outputs = []\n",
    "        decoder_input = torch.zeros(batch_size, 80).to(device)  # 시작 프레임\n",
    "\n",
    "        # 마스크 생성\n",
    "        memory_lengths = input_lengths\n",
    "        mask = ~get_mask_from_lengths(memory_lengths, max_len=encoder_outputs.size(1)).to(device)\n",
    "\n",
    "        for t in range(max_decoder_steps):\n",
    "            prenet_output = model.decoder.prenet(decoder_input)\n",
    "            attention_rnn_input = torch.cat((prenet_output, attention_context), dim=-1)\n",
    "            attention_hidden, attention_cell = model.decoder.attention_rnn(attention_rnn_input, (attention_hidden, attention_cell))\n",
    "            attention_context, attention_weights = model.decoder.attention_layer(attention_hidden, encoder_outputs, mask)\n",
    "            decoder_rnn_input = torch.cat((attention_hidden, attention_context), dim=-1)\n",
    "            decoder_hidden, decoder_cell = model.decoder.decoder_rnn(decoder_rnn_input, (decoder_hidden, decoder_cell))\n",
    "            decoder_output = torch.cat((decoder_hidden, attention_context), dim=-1)\n",
    "            mel_output = model.decoder.linear_projection(decoder_output)\n",
    "            stop_output = model.decoder.stop_projection(decoder_output)\n",
    "            mel_outputs.append(mel_output.unsqueeze(1))\n",
    "            stop_outputs.append(stop_output)\n",
    "\n",
    "            # 종료 조건 확인\n",
    "            if torch.sigmoid(stop_output) > 0.5:\n",
    "                break\n",
    "\n",
    "            decoder_input = mel_output\n",
    "\n",
    "        mel_outputs = torch.cat(mel_outputs, dim=1)  # (batch_size, time_steps, 80)\n",
    "        mel_outputs_postnet = mel_outputs + model.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs_postnet.squeeze(0).cpu().numpy()\n",
    "        return mel_outputs_postnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc8caf258a4a817f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:43:49.502721Z",
     "start_time": "2024-11-19T14:42:50.587532Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seungju\\Python\\Python311\\Lib\\site-packages\\torch\\hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/nvidia/DeepLearningExamples/zipball/torchhub\" to C:\\Users\\Seungju/.cache\\torch\\hub\\torchhub.zip\n",
      "C:\\Users\\Seungju/.cache\\torch\\hub\\nvidia_DeepLearningExamples_torchhub\\PyTorch\\Classification\\ConvNets\\image_classification\\models\\common.py:13: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "C:\\Users\\Seungju/.cache\\torch\\hub\\nvidia_DeepLearningExamples_torchhub\\PyTorch\\Classification\\ConvNets\\image_classification\\models\\efficientnet.py:17: UserWarning: pytorch_quantization module not found, quantization will not be available\n",
      "  warnings.warn(\n",
      "Downloading checkpoint from https://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ckpt_fp32/versions/19.09.0/files/nvidia_waveglowpyt_fp32_20190427\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 예시: WaveGlow를 사용한 음성 합성\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# WaveGlow 모델 로드 (사전 학습된 모델 필요)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m waveglow \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhub\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnvidia/DeepLearningExamples:torchhub\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnvidia_waveglow\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      4\u001b[0m waveglow\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmel_to_audio\u001b[39m(mel):\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\site-packages\\torch\\hub.py:647\u001b[0m, in \u001b[0;36mload\u001b[1;34m(repo_or_dir, model, source, trust_repo, force_reload, verbose, skip_validation, *args, **kwargs)\u001b[0m\n\u001b[0;32m    637\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgithub\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    638\u001b[0m     repo_or_dir \u001b[38;5;241m=\u001b[39m _get_cache_or_reload(\n\u001b[0;32m    639\u001b[0m         repo_or_dir,\n\u001b[0;32m    640\u001b[0m         force_reload,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    644\u001b[0m         skip_validation\u001b[38;5;241m=\u001b[39mskip_validation,\n\u001b[0;32m    645\u001b[0m     )\n\u001b[1;32m--> 647\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43m_load_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_or_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\site-packages\\torch\\hub.py:676\u001b[0m, in \u001b[0;36m_load_local\u001b[1;34m(hubconf_dir, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    673\u001b[0m     hub_module \u001b[38;5;241m=\u001b[39m _import_module(MODULE_HUBCONF, hubconf_path)\n\u001b[0;32m    675\u001b[0m     entry \u001b[38;5;241m=\u001b[39m _load_entry_from_hubconf(hub_module, model)\n\u001b[1;32m--> 676\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mentry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\nvidia_DeepLearningExamples_torchhub\\PyTorch\\SpeechSynthesis\\Tacotron2\\waveglow\\entrypoints.py:95\u001b[0m, in \u001b[0;36mnvidia_waveglow\u001b[1;34m(pretrained, **kwargs)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     94\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://api.ngc.nvidia.com/v2/models/nvidia/waveglow_ckpt_fp32/versions/19.09.0/files/nvidia_waveglowpyt_fp32_20190427\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 95\u001b[0m ckpt_file \u001b[38;5;241m=\u001b[39m \u001b[43m_download_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_reload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m ckpt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(ckpt_file)\n\u001b[0;32m     97\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m ckpt[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstate_dict\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~/.cache\\torch\\hub\\nvidia_DeepLearningExamples_torchhub\\PyTorch\\SpeechSynthesis\\Tacotron2\\waveglow\\entrypoints.py:73\u001b[0m, in \u001b[0;36m_download_checkpoint\u001b[1;34m(checkpoint, force_reload)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(ckpt_file) \u001b[38;5;129;01mor\u001b[39;00m force_reload:\n\u001b[0;32m     72\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownloading checkpoint from \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(checkpoint))\n\u001b[1;32m---> 73\u001b[0m     \u001b[43murllib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ckpt_file\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\urllib\\request.py:270\u001b[0m, in \u001b[0;36murlretrieve\u001b[1;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[0;32m    267\u001b[0m     reporthook(blocknum, bs, size)\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 270\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m block:\n\u001b[0;32m    272\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\http\\client.py:473\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[0;32m    472\u001b[0m     amt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength\n\u001b[1;32m--> 473\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mread(amt)\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[0;32m    476\u001b[0m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_conn()\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32m~\\Python\\Python311\\Lib\\ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 예시: WaveGlow를 사용한 음성 합성\n",
    "# WaveGlow 모델 로드 (사전 학습된 모델 필요)\n",
    "waveglow = torch.hub.load('nvidia/DeepLearningExamples:torchhub', 'nvidia_waveglow').to(device)\n",
    "waveglow.eval()\n",
    "\n",
    "def mel_to_audio(mel):\n",
    "    mel = torch.from_numpy(mel).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        audio = waveglow.infer(mel)\n",
    "    audio = audio.cpu().numpy()[0]\n",
    "    return audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d382f7a609ab49e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T14:51:22.800212Z",
     "start_time": "2024-11-19T14:51:22.786189Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m안녕하세요. 타코트론2 모델을 사용하여 음성을 합성합니다.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 멜 스펙트로그램 생성\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m mel_output \u001b[38;5;241m=\u001b[39m synthesize(\u001b[43mmodel\u001b[49m, text, vocab)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 음성 신호로 변환\u001b[39;00m\n\u001b[0;32m      8\u001b[0m audio \u001b[38;5;241m=\u001b[39m mel_to_audio(mel_output)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# 텍스트 입력\n",
    "text = \"안녕하세요. 타코트론2 모델을 사용하여 음성을 합성합니다.\"\n",
    "\n",
    "# 멜 스펙트로그램 생성\n",
    "mel_output = synthesize(model, text, vocab)\n",
    "\n",
    "# 음성 신호로 변환\n",
    "audio = mel_to_audio(mel_output)\n",
    "\n",
    "# 오디오 저장\n",
    "import soundfile as sf\n",
    "sf.write('output.wav', audio, 22050)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
