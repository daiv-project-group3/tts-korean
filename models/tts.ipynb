{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4307d819d71e6b74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:12.261698Z",
     "start_time": "2024-11-26T06:37:12.256401Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared_hdd/shared_hdd/haesol1013/anaconda3/envs/py311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import librosa\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm, spectral_norm\n",
    "import torchaudio\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:12.887202Z",
     "start_time": "2024-11-26T06:37:12.858049Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.set_device(config.device_num)\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        \n",
    "        # 데이터셋 및 데이터로더 초기화\n",
    "        self.train_dataset = KSSTTSDataset(split='train')\n",
    "        self.valid_dataset = KSSTTSDataset(split='valid')\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=KSSTTSDataset.collate_fn,\n",
    "            num_workers=4\n",
    "        )\n",
    "        \n",
    "        self.valid_loader = DataLoader(\n",
    "            self.valid_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=KSSTTSDataset.collate_fn,\n",
    "            num_workers=4\n",
    "        )\n",
    "        \n",
    "        # 모델 초기화\n",
    "        self.tacotron2 = Tacotron2(\n",
    "            n_mel_channels=80,\n",
    "            vocab_size=len(self.train_dataset.vocab),\n",
    "            embedding_dim=256,\n",
    "            encoder_n_convolutions=3,\n",
    "            encoder_kernel_size=5,\n",
    "            attention_rnn_dim=512,\n",
    "            attention_dim=128\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.generator = Generator().to(self.device)\n",
    "        self.msd = MultiScaleDiscriminator().to(self.device)\n",
    "        self.mpd = MultiPeriodDiscriminator().to(self.device)\n",
    "        self.mrf = MRFDiscriminator().to(self.device)\n",
    "        \n",
    "        # Loss 함수 초기화\n",
    "        self.criterion = TTSLoss(device=self.device)\n",
    "        \n",
    "        # Optimizer 초기화\n",
    "        self.optimizer_g = optim.AdamW([\n",
    "            {'params': self.tacotron2.parameters()},\n",
    "            {'params': self.generator.parameters()}\n",
    "        ], lr=config.learning_rate, betas=(0.8, 0.99), weight_decay=0.01)\n",
    "        \n",
    "        self.optimizer_d = optim.AdamW([\n",
    "            {'params': self.msd.parameters()},\n",
    "            {'params': self.mpd.parameters()},\n",
    "            {'params': self.mrf.parameters()}\n",
    "        ], lr=config.learning_rate, betas=(0.8, 0.99), weight_decay=0.01)\n",
    "        \n",
    "        # 체크포인트 디렉토리 생성\n",
    "        self.checkpoint_dir = Path(config.checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def save_checkpoint(self, epoch, loss):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'tacotron2_state_dict': self.tacotron2.state_dict(),\n",
    "            'generator_state_dict': self.generator.state_dict(),\n",
    "            'msd_state_dict': self.msd.state_dict(),\n",
    "            'mpd_state_dict': self.mpd.state_dict(),\n",
    "            'mrf_state_dict': self.mrf.state_dict(),\n",
    "            'optimizer_t2_state_dict': self.optimizer_t2.state_dict(),\n",
    "            'optimizer_g_state_dict': self.optimizer_g.state_dict(),\n",
    "            'optimizer_d_state_dict': self.optimizer_d.state_dict(),\n",
    "            'loss': loss\n",
    "        }\n",
    "        \n",
    "        path = self.checkpoint_dir / f'checkpoint_epoch_{epoch}.pt'\n",
    "        torch.save(checkpoint, path)\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        \n",
    "        self.tacotron2.load_state_dict(checkpoint['tacotron2_state_dict'])\n",
    "        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        self.msd.load_state_dict(checkpoint['msd_state_dict'])\n",
    "        self.mpd.load_state_dict(checkpoint['mpd_state_dict'])\n",
    "        self.mrf.load_state_dict(checkpoint['mrf_state_dict'])\n",
    "        \n",
    "        self.optimizer_t2.load_state_dict(checkpoint['optimizer_t2_state_dict'])\n",
    "        self.optimizer_g.load_state_dict(checkpoint['optimizer_g_state_dict'])\n",
    "        self.optimizer_d.load_state_dict(checkpoint['optimizer_d_state_dict'])\n",
    "        \n",
    "        return checkpoint['epoch']\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        try:\n",
    "            # 배치 데이터 언패킹\n",
    "            text_padded = batch['text_padded'].to(self.device)\n",
    "            mel_padded = batch['mel_padded'].to(self.device)\n",
    "            gate_padded = batch['gate_padded'].to(self.device)\n",
    "            audio_padded = batch['audio_padded'].to(self.device)\n",
    "            text_lengths = batch['text_lengths'].to(self.device)\n",
    "            mel_lengths = batch['mel_lengths'].to(self.device)\n",
    "            \n",
    "            # 디버깅을 위한 shape 출력\n",
    "            print(\"\\nInput shapes:\")\n",
    "            print(f\"text_padded: {text_padded.shape}\")\n",
    "            print(f\"mel_padded: {mel_padded.shape}\")\n",
    "            print(f\"gate_padded: {gate_padded.shape}\")\n",
    "            print(f\"audio_padded: {audio_padded.shape}\")\n",
    "            \n",
    "            # Tacotron2 forward\n",
    "            mel_outputs_postnet, mel_outputs, gate_outputs, _ = self.tacotron2(\n",
    "                text_padded, text_lengths, mel_padded, mel_lengths)\n",
    "            \n",
    "            # Generator forward\n",
    "            mel_for_generator = mel_outputs_postnet.transpose(1, 2)\n",
    "            fake_audio = self.generator(mel_for_generator)\n",
    "            \n",
    "            # Discriminator forward\n",
    "            msd_real_outputs, msd_real_features = self.msd(audio_padded)\n",
    "            msd_fake_outputs, msd_fake_features = self.msd(fake_audio.detach())\n",
    "            \n",
    "            mpd_real_outputs, mpd_real_features = self.mpd(audio_padded)\n",
    "            mpd_fake_outputs, mpd_fake_features = self.mpd(fake_audio.detach())\n",
    "            \n",
    "            # Loss 계산\n",
    "            # Tacotron2 loss\n",
    "            tacotron2_losses = self.criterion.tacotron2_loss(\n",
    "                mel_outputs, mel_outputs_postnet, gate_outputs,\n",
    "                mel_padded, gate_padded, mel_lengths\n",
    "            )\n",
    "            \n",
    "            # HiFi-GAN loss\n",
    "            real_outputs = msd_real_outputs + mpd_real_outputs\n",
    "            fake_outputs = msd_fake_outputs + mpd_fake_outputs\n",
    "            real_features = msd_real_features + mpd_real_features\n",
    "            fake_features = msd_fake_features + mpd_fake_features\n",
    "            \n",
    "            hifigan_losses = self.criterion.hifi_gan_loss(\n",
    "                audio_padded, fake_audio,\n",
    "                real_outputs, fake_outputs,\n",
    "                real_features, fake_features\n",
    "            )\n",
    "            \n",
    "            # Optimizer step\n",
    "            # Generator update\n",
    "            self.optimizer_g.zero_grad()\n",
    "            g_loss = tacotron2_losses['total_loss'] + hifigan_losses['generator_loss']\n",
    "            g_loss.backward(retain_graph=True)\n",
    "            self.optimizer_g.step()\n",
    "            \n",
    "            # Discriminator update\n",
    "            self.optimizer_d.zero_grad()\n",
    "            d_loss = hifigan_losses['discriminator_loss']\n",
    "            d_loss.backward()\n",
    "            self.optimizer_d.step()\n",
    "            \n",
    "            return {\n",
    "                'total_loss': g_loss + d_loss,\n",
    "                **tacotron2_losses,\n",
    "                **hifigan_losses\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in train_step: {str(e)}\")\n",
    "            print(\"\\nBatch information:\")\n",
    "            print(f\"Batch size: {len(batch)}\")\n",
    "            print(f\"Batch contents: {[type(x) for x in batch]}\")\n",
    "            raise e\n",
    "\n",
    "    def train(self):\n",
    "        start_epoch = 0\n",
    "        if self.config.resume_checkpoint:\n",
    "            start_epoch = self.load_checkpoint(self.config.resume_checkpoint)\n",
    "        \n",
    "        for epoch in range(start_epoch, self.config.num_epochs):\n",
    "            self.tacotron2.train()\n",
    "            self.generator.train()\n",
    "            self.msd.train()\n",
    "            self.mpd.train()\n",
    "            self.mrf.train()\n",
    "            \n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(self.train_loader)\n",
    "            \n",
    "            for batch in progress_bar:\n",
    "                losses = self.train_step(batch)\n",
    "                total_loss += losses['total_loss'].item()\n",
    "                \n",
    "                progress_bar.set_description(\n",
    "                    f\"Epoch {epoch+1}, Loss: {losses['total_loss'].item():.4f}\"\n",
    "                )\n",
    "            \n",
    "            avg_loss = total_loss / len(self.train_loader)\n",
    "            \n",
    "            if (epoch + 1) % self.config.save_interval == 0:\n",
    "                self.save_checkpoint(epoch + 1, avg_loss)\n",
    "            \n",
    "            print(f'Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bb0e5859b3e6d1fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:13.291211Z",
     "start_time": "2024-11-26T06:37:13.268245Z"
    }
   },
   "outputs": [],
   "source": [
    "class KSSTTSDataset(Dataset):\n",
    "    def __init__(self, split='train', valid_size=0.1, seed=42, target_sr=22050):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 기본 설정\n",
    "        self.target_sr = target_sr\n",
    "        self.vocab = None\n",
    "        \n",
    "        # 데이터셋 로드\n",
    "        dataset = load_dataset(\"Bingsu/KSS_Dataset\")\n",
    "        full_dataset = dataset['train']\n",
    "        \n",
    "        # train/valid 분할\n",
    "        train_idx, valid_idx = train_test_split(\n",
    "            range(len(full_dataset)),\n",
    "            test_size=valid_size,\n",
    "            random_state=seed\n",
    "        )\n",
    "        \n",
    "        self.indices = train_idx if split == 'train' else valid_idx\n",
    "        self.dataset = full_dataset\n",
    "        \n",
    "        # vocab 초기화\n",
    "        self._initialize_vocab()\n",
    "        \n",
    "        # Mel spectrogram 변환을 위한 transform 초기화\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=target_sr,\n",
    "            n_fft=1024,\n",
    "            hop_length=256,\n",
    "            n_mels=80,\n",
    "            f_min=0,\n",
    "            f_max=8000,\n",
    "            power=1\n",
    "        )\n",
    "        \n",
    "        # log mel spectrogram을 위한 처리\n",
    "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        \n",
    "    def _initialize_vocab(self):\n",
    "        all_texts = [self.dataset[idx]['original_script'] for idx in self.indices]\n",
    "        unique_tokens = set()\n",
    "        \n",
    "        for text in all_texts:\n",
    "            sequence = self._text_to_sequence(text)\n",
    "            unique_tokens.update(sequence)\n",
    "            \n",
    "        self.vocab = {token: idx for idx, token in enumerate(sorted(unique_tokens))}\n",
    "        self.vocab['<pad>'] = len(self.vocab)\n",
    "        self.vocab['<unk>'] = len(self.vocab)\n",
    "    \n",
    "    def _text_to_sequence(self, text):\n",
    "        # 자모 분해 없이 텍스트를 그대로 시퀀스로 변환\n",
    "        return list(text)\n",
    "    \n",
    "    def _wav_to_mel(self, wav):\n",
    "        stft = librosa.stft(wav, n_fft=1024, hop_length=256, win_length=1024)\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            S=np.abs(stft)**2,\n",
    "            sr=self.target_sr,\n",
    "            n_mels=80,\n",
    "            fmin=0,\n",
    "            fmax=8000\n",
    "        )\n",
    "        mel_spec = np.log(np.clip(mel_spec, a_min=1e-5, a_max=None))\n",
    "        mel_spec = np.clip(mel_spec, a_min=-4, a_max=4)\n",
    "        mel_spec = (mel_spec + 4) / 8\n",
    "        return mel_spec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.indices[idx]\n",
    "        item = self.dataset[real_idx]\n",
    "        \n",
    "        # 텍스트 처리\n",
    "        text = item['decomposed_script']\n",
    "        sequence = self._text_to_sequence(text)\n",
    "        sequence = [self.vocab.get(char, self.vocab['<unk>']) for char in sequence]\n",
    "        sequence = torch.LongTensor(sequence)\n",
    "        \n",
    "        # 오디오 처리\n",
    "        audio = torch.FloatTensor(item['audio']['array']).unsqueeze(0)  # [1, T]\n",
    "        \n",
    "        # 오디오 길이 제한 (5초로 축소)\n",
    "        max_audio_length = 7 * self.target_sr\n",
    "        if audio.size(-1) > max_audio_length:\n",
    "            start = torch.randint(0, audio.size(-1) - max_audio_length, (1,))\n",
    "            audio = audio[:, start:start + max_audio_length]\n",
    "        \n",
    "        # Mel spectrogram 계산 전 오디오 다운샘플링\n",
    "        if self.target_sr > 16000:\n",
    "            audio = torchaudio.transforms.Resample(\n",
    "                self.target_sr, 16000\n",
    "            )(audio)\n",
    "            self.target_sr = 16000\n",
    "        \n",
    "        # 멜스펙트로그램 계산\n",
    "        mel = self.mel_transform(audio)  # [n_mels, T]\n",
    "        mel = self.amplitude_to_db(mel)\n",
    "        \n",
    "        # 정규화\n",
    "        mel = (mel + 80) / 80\n",
    "        \n",
    "        # print(f\"Mel shape in __getitem__: {mel.shape}\")\n",
    "        \n",
    "        return {\n",
    "            'text': sequence,\n",
    "            'mel': mel,  # [n_mels, T] 형태로 반환\n",
    "            'audio': audio\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        # 텍스트 처리\n",
    "        input_lengths = [len(x['text']) for x in batch]\n",
    "        max_input_len = max(input_lengths)\n",
    "        \n",
    "        text_padded = torch.zeros(len(batch), max_input_len, dtype=torch.long)\n",
    "        for i, x in enumerate(batch):\n",
    "            text = x['text']\n",
    "            text_padded[i, :len(text)] = text\n",
    "\n",
    "        # 멜스펙트로그램 처리\n",
    "        mel_lengths = [x['mel'].size(-1) for x in batch]\n",
    "        max_mel_len = max(mel_lengths)\n",
    "        \n",
    "        # mel_padded 차원 수정 [batch_size, n_mels, time]\n",
    "        mel_padded = torch.zeros(len(batch), 80, max_mel_len)\n",
    "        gate_padded = torch.zeros(len(batch), max_mel_len)\n",
    "        \n",
    "        for i, x in enumerate(batch):\n",
    "            mel = x['mel']\n",
    "            # mel의 shape 출력\n",
    "            # print(f\"Mel shape before processing: {mel.shape}\")\n",
    "            \n",
    "            # mel의 shape를 [batch_size, n_mels, time]으로 변경\n",
    "            if mel.dim() == 3:  # [1, 80, T]\n",
    "                mel = mel.squeeze(0)  # [80, T]\n",
    "            elif mel.dim() == 2 and mel.size(0) != 80:  # [T, 80]\n",
    "                mel = mel.transpose(0, 1)  # [80, T]\n",
    "                \n",
    "            # print(f\"Mel shape after processing: {mel.shape}\")\n",
    "            \n",
    "            # 패딩\n",
    "            cur_len = mel.size(1)\n",
    "            mel_padded[i, :, :cur_len] = mel\n",
    "            gate_padded[i, cur_len-1:] = 1\n",
    "\n",
    "        # 오디오 처리\n",
    "        audio_lengths = [x['audio'].size(-1) for x in batch]\n",
    "        max_audio_len = max(audio_lengths)\n",
    "        \n",
    "        audio_padded = torch.zeros(len(batch), 1, max_audio_len)\n",
    "        for i, x in enumerate(batch):\n",
    "            audio = x['audio']\n",
    "            if audio.dim() == 1:\n",
    "                audio = audio.unsqueeze(0)\n",
    "            audio_padded[i, :, :audio.size(-1)] = audio\n",
    "\n",
    "        return (text_padded, torch.LongTensor(input_lengths),\n",
    "                mel_padded, gate_padded, torch.LongTensor(mel_lengths),\n",
    "                audio_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7639d50c4afaf64a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:13.590133Z",
     "start_time": "2024-11-26T06:37:13.581889Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        # 패딩 계산을 동적으로 수행\n",
    "        self.padding = (dilation * (kernel_size - 1)) // 2\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels, out_channels, kernel_size,\n",
    "            stride=stride, padding=self.padding, dilation=dilation\n",
    "        )\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            out_channels, out_channels, kernel_size,\n",
    "            stride=stride, padding=self.padding, dilation=dilation\n",
    "        )\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        self.skip_connection = (\n",
    "            nn.Conv1d(in_channels, out_channels, 1)\n",
    "            if in_channels != out_channels else\n",
    "            nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.skip_connection(x)\n",
    "        x = self.leaky_relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        # 차원이 다른 경우 residual을 조정\n",
    "        if x.size(-1) != residual.size(-1):\n",
    "            # 더 작은 크기에 맞춤\n",
    "            target_size = min(x.size(-1), residual.size(-1))\n",
    "            x = x[..., :target_size]\n",
    "            residual = residual[..., :target_size]\n",
    "            \n",
    "        return self.leaky_relu(x + residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3dcaac7ab138c26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:13.835611Z",
     "start_time": "2024-11-26T06:37:13.820817Z"
    }
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size=80):\n",
    "        super().__init__()\n",
    "        # Initial conv\n",
    "        self.conv_pre = nn.Conv1d(input_size, 512, 7, 1, 3)\n",
    "        \n",
    "        # Upsampling layers with kernel size and stride adjustments\n",
    "        self.ups = nn.ModuleList([\n",
    "            nn.ConvTranspose1d(512, 256, 16, 8, 4),\n",
    "            nn.ConvTranspose1d(256, 128, 16, 8, 4),\n",
    "            nn.ConvTranspose1d(128, 64, 4, 2, 1),\n",
    "            nn.ConvTranspose1d(64, 32, 4, 2, 1),\n",
    "        ])\n",
    "        \n",
    "        # Multi-Receptive Field Fusion\n",
    "        self.mrf_blocks = nn.ModuleList([\n",
    "            # First MRF block\n",
    "            nn.ModuleList([\n",
    "                ResBlock(256, 256, kernel_size=3, dilation=1),\n",
    "                ResBlock(256, 256, kernel_size=3, dilation=3),\n",
    "                ResBlock(256, 256, kernel_size=3, dilation=5)\n",
    "            ]),\n",
    "            # Second MRF block\n",
    "            nn.ModuleList([\n",
    "                ResBlock(128, 128, kernel_size=3, dilation=1),\n",
    "                ResBlock(128, 128, kernel_size=3, dilation=3),\n",
    "                ResBlock(128, 128, kernel_size=3, dilation=5)\n",
    "            ]),\n",
    "            # Third MRF block\n",
    "            nn.ModuleList([\n",
    "                ResBlock(64, 64, kernel_size=3, dilation=1),\n",
    "                ResBlock(64, 64, kernel_size=3, dilation=3)\n",
    "            ]),\n",
    "            # Fourth MRF block\n",
    "            nn.ModuleList([\n",
    "                ResBlock(32, 32, kernel_size=3, dilation=1),\n",
    "                ResBlock(32, 32, kernel_size=3, dilation=2)\n",
    "            ])\n",
    "        ])\n",
    "        \n",
    "        # Final conv\n",
    "        self.conv_post = nn.Conv1d(32, 1, 7, 1, 3)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Generator input shape: {x.shape}\")\n",
    "        x = self.conv_pre(x)\n",
    "        print(f\"After conv_pre: {x.shape}\")\n",
    "        \n",
    "        for i in range(len(self.ups)):\n",
    "            x = self.leaky_relu(x)\n",
    "            x = self.ups[i](x)\n",
    "            print(f\"After up {i}: {x.shape}\")\n",
    "            \n",
    "            # Apply MRF blocks\n",
    "            xs = None\n",
    "            for j, resblock in enumerate(self.mrf_blocks[i]):\n",
    "                if xs is None:\n",
    "                    xs = resblock(x)\n",
    "                else:\n",
    "                    # 크기가 다른 경우 처리\n",
    "                    res_out = resblock(x)\n",
    "                    if res_out.size(-1) != xs.size(-1):\n",
    "                        target_size = min(res_out.size(-1), xs.size(-1))\n",
    "                        xs = xs[..., :target_size]\n",
    "                        res_out = res_out[..., :target_size]\n",
    "                    xs += res_out\n",
    "            x = xs / len(self.mrf_blocks[i])\n",
    "        \n",
    "        print(f\"Generator output shape: {x.shape}\")\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv_post(x)\n",
    "        x = torch.tanh(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5c5d1fdb30962d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:14.043329Z",
     "start_time": "2024-11-26T06:37:14.037012Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiScaleDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.discriminators = nn.ModuleList([\n",
    "            DiscriminatorS(use_spectral_norm=True),\n",
    "            DiscriminatorS(),\n",
    "            DiscriminatorS(),\n",
    "        ])\n",
    "        self.meanpools = nn.ModuleList([\n",
    "            nn.AvgPool1d(4, 2, padding=2),\n",
    "            nn.AvgPool1d(4, 2, padding=2)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: 입력 오디오 (fake 또는 real)\n",
    "        \"\"\"\n",
    "        y_d_rs = []  # discriminator outputs\n",
    "        fmap_rs = []  # feature maps\n",
    "\n",
    "        for i, d in enumerate(self.discriminators):\n",
    "            if i != 0:\n",
    "                x = self.meanpools[i-1](x)\n",
    "            y_d_r, fmap_r = d(x)\n",
    "            y_d_rs.append(y_d_r)\n",
    "            fmap_rs.append(fmap_r)\n",
    "\n",
    "        return y_d_rs, fmap_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "82a670b22e6ab8f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:14.258300Z",
     "start_time": "2024-11-26T06:37:14.249974Z"
    }
   },
   "outputs": [],
   "source": [
    "class DiscriminatorS(nn.Module):\n",
    "    def __init__(self, use_spectral_norm=False):\n",
    "        super().__init__()\n",
    "        norm_f = weight_norm if not use_spectral_norm else spectral_norm\n",
    "        self.convs = nn.ModuleList([\n",
    "            norm_f(nn.Conv1d(1, 128, 15, 1, padding=7)),\n",
    "            norm_f(nn.Conv1d(128, 128, 41, 2, groups=4, padding=20)),\n",
    "            norm_f(nn.Conv1d(128, 256, 41, 2, groups=16, padding=20)),\n",
    "            norm_f(nn.Conv1d(256, 512, 41, 4, groups=16, padding=20)),\n",
    "            norm_f(nn.Conv1d(512, 1024, 41, 4, groups=16, padding=20)),\n",
    "            norm_f(nn.Conv1d(1024, 1024, 41, 1, groups=16, padding=20)),\n",
    "            norm_f(nn.Conv1d(1024, 1024, 5, 1, padding=2)),\n",
    "        ])\n",
    "        self.conv_post = norm_f(nn.Conv1d(1024, 1, 3, 1, padding=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = []\n",
    "        for l in self.convs:\n",
    "            x = l(x)\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            fmap.append(x)\n",
    "        x = self.conv_post(x)\n",
    "        fmap.append(x)\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "        return x, fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d25351488311bb23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:14.486401Z",
     "start_time": "2024-11-26T06:37:14.476736Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiPeriodDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.discriminators = nn.ModuleList([\n",
    "            DiscriminatorP(2),\n",
    "            DiscriminatorP(3),\n",
    "            DiscriminatorP(5),\n",
    "            DiscriminatorP(7),\n",
    "            DiscriminatorP(11),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: 입력 오디오 (fake 또는 real)\n",
    "        \"\"\"\n",
    "        y_d_rs = []  # discriminator outputs\n",
    "        fmap_rs = []  # feature maps\n",
    "\n",
    "        for d in self.discriminators:\n",
    "            y_d_r, fmap_r = d(x)\n",
    "            y_d_rs.append(y_d_r)\n",
    "            fmap_rs.append(fmap_r)\n",
    "\n",
    "        return y_d_rs, fmap_rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "157c8527b4139627",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:14.827720Z",
     "start_time": "2024-11-26T06:37:14.816894Z"
    }
   },
   "outputs": [],
   "source": [
    "class DiscriminatorP(nn.Module):\n",
    "    def __init__(self, period):\n",
    "        super().__init__()\n",
    "        self.period = period\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            weight_norm(nn.Conv2d(1, 32, (5, 1), (3, 1), padding=(2, 0))),\n",
    "            weight_norm(nn.Conv2d(32, 128, (5, 1), (3, 1), padding=(2, 0))),\n",
    "            weight_norm(nn.Conv2d(128, 512, (5, 1), (3, 1), padding=(2, 0))),\n",
    "            weight_norm(nn.Conv2d(512, 1024, (5, 1), (3, 1), padding=(2, 0))),\n",
    "            weight_norm(nn.Conv2d(1024, 1024, (5, 1), 1, padding=(2, 0))),\n",
    "        ])\n",
    "        self.conv_post = weight_norm(nn.Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, 1, T]\n",
    "        \"\"\"\n",
    "        fmap = []\n",
    "        \n",
    "        # 1D -> 2D\n",
    "        b, c, t = x.shape\n",
    "        if t % self.period != 0:  # 패딩 추가\n",
    "            n_pad = self.period - (t % self.period)\n",
    "            x = F.pad(x, (0, n_pad), \"reflect\")\n",
    "            t = t + n_pad\n",
    "        x = x.view(b, c, t // self.period, self.period)\n",
    "\n",
    "        for layer in self.convs:\n",
    "            x = layer(x)\n",
    "            x = self.leaky_relu(x)\n",
    "            fmap.append(x)\n",
    "        x = self.conv_post(x)\n",
    "        fmap.append(x)\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "\n",
    "        return x, fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1653e2b27914b023",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:15.049102Z",
     "start_time": "2024-11-26T06:37:15.038213Z"
    }
   },
   "outputs": [],
   "source": [
    "class MRFDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 초기 채널 조정을 위한 Conv1d 레이어들\n",
    "        self.msd_adjust = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # MPD feature를 위한 2D -> 1D 변환 레이어\n",
    "        self.mpd_adjust = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        \n",
    "        # 공통 처리를 위한 레이어들\n",
    "        self.shared_conv = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv1d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv1d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        \n",
    "        # 최종 출력을 위한 레이어\n",
    "        self.output_layer = nn.Conv1d(512, 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, msd_features, mpd_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            msd_features: List[List[Tensor]] - MultiScaleDiscriminator의 feature maps\n",
    "            mpd_features: List[List[Tensor]] - MultiPeriodDiscriminator의 feature maps\n",
    "        \"\"\"\n",
    "        # 마지막 레이어의 feature map 사용\n",
    "        msd_feat = msd_features[-1][-1]  # [B, 1, T]\n",
    "        mpd_feat = mpd_features[-1][-1]  # [B, 1, H, W]\n",
    "        \n",
    "        # MPD feature 처리\n",
    "        if mpd_feat.dim() == 4:\n",
    "            B, C, H, W = mpd_feat.size()\n",
    "            # 2D 처리\n",
    "            mpd_processed = self.mpd_adjust(mpd_feat)  # [B, 64, H, W]\n",
    "            # Flatten H, W 차원\n",
    "            mpd_processed = mpd_processed.view(B, 64, -1)  # [B, 64, H*W]\n",
    "        \n",
    "        # MSD feature 처리\n",
    "        msd_processed = self.msd_adjust(msd_feat)  # [B, 64, T]\n",
    "        \n",
    "        # 시간 차원 맞추기\n",
    "        target_length = min(msd_processed.size(-1), mpd_processed.size(-1))\n",
    "        msd_processed = F.interpolate(msd_processed, size=target_length, mode='linear', align_corners=False)\n",
    "        mpd_processed = F.interpolate(mpd_processed, size=target_length, mode='linear', align_corners=False)\n",
    "        \n",
    "        # Feature map 결합\n",
    "        combined = torch.cat([msd_processed, mpd_processed], dim=1)  # [B, 128, T]\n",
    "        \n",
    "        # 공통 처리\n",
    "        x = self.shared_conv(combined)\n",
    "        \n",
    "        # 최종 출력\n",
    "        output = self.output_layer(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59dde7bd446e9993",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:15.239217Z",
     "start_time": "2024-11-26T06:37:15.217785Z"
    }
   },
   "outputs": [],
   "source": [
    "class TTSLoss(nn.Module):\n",
    "    def __init__(self, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # Loss weights from the paper\n",
    "        self.lambda_mel = 45.0\n",
    "        self.lambda_fm = 2.0\n",
    "        self.lambda_adv = 1.0\n",
    "        \n",
    "        # Mel-spectrogram transform configuration\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=22050,\n",
    "            n_fft=1024,\n",
    "            win_length=1024,\n",
    "            hop_length=256,\n",
    "            f_min=0,\n",
    "            f_max=8000,\n",
    "            n_mels=80,\n",
    "            power=1.0,\n",
    "            normalized=True\n",
    "        ).to(device)\n",
    "\n",
    "    def tacotron2_loss(self, mel_output, mel_output_postnet, gate_out, \n",
    "                      mel_target, gate_target, mel_lengths):\n",
    "        \"\"\"Tacotron2 Loss Calculation\"\"\"\n",
    "        # mel_target: [B, n_mel_channels, T]\n",
    "        # mel_output: [B, T, n_mel_channels]\n",
    "        \n",
    "        # 차원 맞추기\n",
    "        mel_target = mel_target.transpose(1, 2)  # [B, T, n_mel_channels]\n",
    "        \n",
    "        # gate_out 차원 조정 [B, T, 1] -> [B, T]\n",
    "        gate_out = gate_out.squeeze(-1)\n",
    "        \n",
    "        # 텐서 차원 확인\n",
    "        B, T, C = mel_target.size()  # [batch_size, time_steps, n_mel_channels]\n",
    "        \n",
    "        # 마스크 생성 (B, T)\n",
    "        mask = ~self.get_mask_from_lengths(mel_lengths)\n",
    "        \n",
    "        # 마스크를 멜 스펙트로그램 차원에 맞게 조정\n",
    "        mel_mask = mask.unsqueeze(-1).expand(-1, -1, C)  # [B, T, C]\n",
    "        \n",
    "        # 마스킹 적용\n",
    "        mel_target_masked = mel_target.masked_fill(mel_mask, 0)\n",
    "        mel_output_masked = mel_output.masked_fill(mel_mask, 0)\n",
    "        mel_output_postnet_masked = mel_output_postnet.masked_fill(mel_mask, 0)\n",
    "        \n",
    "        # gate에 대한 마스킹 적용\n",
    "        gate_target = gate_target.masked_fill(mask, 0)\n",
    "        gate_out = gate_out.masked_fill(mask, 0)\n",
    "        \n",
    "        # Loss 계산\n",
    "        mel_loss = self.l1_loss(mel_output_masked, mel_target_masked) + \\\n",
    "                  self.l1_loss(mel_output_postnet_masked, mel_target_masked)\n",
    "        gate_loss = self.bce_loss(gate_out, gate_target)\n",
    "        \n",
    "        return {\n",
    "            'mel_loss': mel_loss,\n",
    "            'gate_loss': gate_loss,\n",
    "            'total_loss': mel_loss + gate_loss\n",
    "        }\n",
    "\n",
    "    def mel_spectrogram_loss(self, real_wave, fake_wave):\n",
    "        \"\"\"Mel-spectrogram L1 Loss\"\"\"\n",
    "        # 더 작은 길이에 맞추기\n",
    "        min_length = min(real_wave.size(-1), fake_wave.size(-1))\n",
    "        real_wave = real_wave[..., :min_length]\n",
    "        fake_wave = fake_wave[..., :min_length]\n",
    "        \n",
    "        # Mel spectrogram 생성\n",
    "        real_mel = self.mel_transform(real_wave)\n",
    "        fake_mel = self.mel_transform(fake_wave)\n",
    "        \n",
    "        # 디버깅을 위한 shape 출력\n",
    "        print(f\"Real wave shape: {real_wave.shape}\")\n",
    "        print(f\"Fake wave shape: {fake_wave.shape}\")\n",
    "        print(f\"Real mel shape: {real_mel.shape}\")\n",
    "        print(f\"Fake mel shape: {fake_mel.shape}\")\n",
    "        \n",
    "        return self.l1_loss(fake_mel, real_mel) * self.lambda_mel\n",
    "\n",
    "    def feature_matching_loss(self, fmap_r, fmap_g):\n",
    "        loss = 0\n",
    "        for dr, dg in zip(fmap_r, fmap_g):\n",
    "            for rl, gl in zip(dr, dg):\n",
    "                if isinstance(rl, torch.Tensor) and isinstance(gl, torch.Tensor):\n",
    "                    # 더 작은 크기에 맞추기\n",
    "                    if rl.dim() == gl.dim():\n",
    "                        min_length = min(rl.size(-1), gl.size(-1))\n",
    "                        if rl.dim() == 3:  # MSD features\n",
    "                            rl = rl[..., :min_length]\n",
    "                            gl = gl[..., :min_length]\n",
    "                        elif rl.dim() == 4:  # MPD features\n",
    "                            rl = rl[..., :min_length, :]\n",
    "                            gl = gl[..., :min_length, :]\n",
    "                        \n",
    "                        loss += torch.mean(torch.abs(rl - gl))\n",
    "        \n",
    "        return loss * self.lambda_fm\n",
    "\n",
    "    def generator_loss(self, disc_outputs):\n",
    "        loss = 0\n",
    "        for dg in disc_outputs:\n",
    "            loss += torch.mean((1-dg)**2)\n",
    "        return loss\n",
    "\n",
    "    def discriminator_loss(self, disc_real_outputs, disc_generated_outputs):\n",
    "        loss = 0\n",
    "        for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
    "            r_loss = torch.mean((1-dr)**2)\n",
    "            g_loss = torch.mean(dg**2)\n",
    "            loss += (r_loss + g_loss)\n",
    "        return loss\n",
    "\n",
    "    def hifi_gan_loss(self, real_wave, fake_wave, real_outputs, fake_outputs, real_feats, fake_feats):\n",
    "        \"\"\"HiFi-GAN Total Loss Calculation\"\"\"\n",
    "        # 차원 확인 및 조정\n",
    "        if real_wave.dim() == 2:\n",
    "            real_wave = real_wave.unsqueeze(1)\n",
    "        if fake_wave.dim() == 2:\n",
    "            fake_wave = fake_wave.unsqueeze(1)\n",
    "        \n",
    "        # 길이 맞추기\n",
    "        min_length = min(real_wave.size(-1), fake_wave.size(-1))\n",
    "        real_wave = real_wave[..., :min_length]\n",
    "        fake_wave = fake_wave[..., :min_length]\n",
    "        \n",
    "        # 디버깅을 위한 shape 출력\n",
    "        print(f\"\\nFeature matching shapes:\")\n",
    "        print(f\"Real features length: {[[(f.shape) for f in disc] for disc in real_feats]}\")\n",
    "        print(f\"Fake features length: {[[(f.shape) for f in disc] for disc in fake_feats]}\")\n",
    "        \n",
    "        # Loss 계산\n",
    "        mel_loss = self.mel_spectrogram_loss(real_wave, fake_wave)\n",
    "        fm_loss = self.feature_matching_loss(real_feats, fake_feats)\n",
    "        gen_loss = self.generator_loss(fake_outputs)\n",
    "        disc_loss = self.discriminator_loss(real_outputs, fake_outputs)\n",
    "        \n",
    "        # Total losses\n",
    "        g_loss = mel_loss + fm_loss + gen_loss\n",
    "        d_loss = disc_loss\n",
    "        \n",
    "        return {\n",
    "            'generator_loss': g_loss,\n",
    "            'discriminator_loss': d_loss,\n",
    "            'mel_loss': mel_loss,\n",
    "            'feature_matching_loss': fm_loss,\n",
    "            'adversarial_loss_g': gen_loss,\n",
    "            'adversarial_loss_d': disc_loss\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mask_from_lengths(lengths):\n",
    "        max_len = torch.max(lengths).item()\n",
    "        ids = torch.arange(0, max_len, device=lengths.device)\n",
    "        mask = (ids < lengths.unsqueeze(1)).bool()\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c61cd5751280f699",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:15.423666Z",
     "start_time": "2024-11-26T06:37:15.416985Z"
    }
   },
   "outputs": [],
   "source": [
    "class LocationLayer(nn.Module):\n",
    "    def __init__(self, attention_n_filters, attention_kernel_size, attention_dim):\n",
    "        super().__init__()\n",
    "        padding = int((attention_kernel_size - 1) / 2)\n",
    "        self.location_conv = nn.Conv1d(2, attention_n_filters, \n",
    "                                     kernel_size=attention_kernel_size,\n",
    "                                     padding=padding, bias=False)\n",
    "        self.location_dense = nn.Linear(attention_n_filters, attention_dim, bias=False)\n",
    "\n",
    "    def forward(self, attention_weights_cat):\n",
    "        processed_attention = self.location_conv(attention_weights_cat)\n",
    "        processed_attention = processed_attention.transpose(1, 2)\n",
    "        processed_attention = self.location_dense(processed_attention)\n",
    "        return processed_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "174f59edc6d1431",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:15.602929Z",
     "start_time": "2024-11-26T06:37:15.595097Z"
    }
   },
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, attention_rnn_dim, embedding_dim, attention_dim,\n",
    "                 attention_location_n_filters, attention_location_kernel_size):\n",
    "        super().__init__()\n",
    "        self.query_layer = nn.Linear(attention_rnn_dim, attention_dim, bias=False)\n",
    "        self.memory_layer = nn.Linear(embedding_dim, attention_dim, bias=False)\n",
    "        self.v = nn.Linear(attention_dim, 1, bias=False)\n",
    "        self.location_layer = LocationLayer(attention_location_n_filters,\n",
    "                                          attention_location_kernel_size,\n",
    "                                          attention_dim)\n",
    "        self.score_mask_value = -float(\"inf\")\n",
    "\n",
    "    def get_alignment_energies(self, query, processed_memory,\n",
    "                             attention_weights_cat):\n",
    "        processed_query = self.query_layer(query.unsqueeze(1))\n",
    "        processed_attention_weights = self.location_layer(attention_weights_cat)\n",
    "        energies = self.v(torch.tanh(\n",
    "            processed_query + processed_memory + processed_attention_weights))\n",
    "        return energies.squeeze(-1)\n",
    "\n",
    "    def forward(self, attention_hidden_state, memory, processed_memory,\n",
    "                attention_weights_cat, mask):\n",
    "        alignment = self.get_alignment_energies(\n",
    "            attention_hidden_state, processed_memory, attention_weights_cat)\n",
    "\n",
    "        if mask is not None:\n",
    "            alignment.data.masked_fill_(mask, self.score_mask_value)\n",
    "\n",
    "        attention_weights = F.softmax(alignment, dim=1)\n",
    "        attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)\n",
    "        attention_context = attention_context.squeeze(1)\n",
    "\n",
    "        return attention_context, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a37d55d6b2b876dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:16.196673Z",
     "start_time": "2024-11-26T06:37:16.189445Z"
    }
   },
   "outputs": [],
   "source": [
    "class Prenet(nn.Module):\n",
    "    def __init__(self, in_dim, sizes):\n",
    "        super().__init__()\n",
    "        in_sizes = [in_dim] + sizes[:-1]\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(in_size, out_size)\n",
    "             for (in_size, out_size) in zip(in_sizes, sizes)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for linear in self.layers:\n",
    "            x = F.dropout(F.relu(linear(x)), p=0.5, training=True)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd3b7defe2ec4875",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:16.739077Z",
     "start_time": "2024-11-26T06:37:16.729199Z"
    }
   },
   "outputs": [],
   "source": [
    "class Postnet(nn.Module):\n",
    "    def __init__(self, n_mel_channels, postnet_embedding_dim,\n",
    "                 postnet_kernel_size, postnet_n_convolutions):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convolutions = nn.ModuleList()\n",
    "        \n",
    "        # 첫 번째 컨볼루션 레이어\n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(n_mel_channels, postnet_embedding_dim,\n",
    "                         kernel_size=postnet_kernel_size, stride=1,\n",
    "                         padding=(postnet_kernel_size - 1) // 2),\n",
    "                nn.BatchNorm1d(postnet_embedding_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(0.5)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 중간 컨볼루션 레이어들\n",
    "        for _ in range(1, postnet_n_convolutions - 1):\n",
    "            self.convolutions.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(postnet_embedding_dim, postnet_embedding_dim,\n",
    "                             kernel_size=postnet_kernel_size, stride=1,\n",
    "                             padding=(postnet_kernel_size - 1) // 2),\n",
    "                    nn.BatchNorm1d(postnet_embedding_dim),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Dropout(0.5)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # 마지막 컨볼루션 레이어\n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(postnet_embedding_dim, n_mel_channels,\n",
    "                         kernel_size=postnet_kernel_size, stride=1,\n",
    "                         padding=(postnet_kernel_size - 1) // 2),\n",
    "                nn.BatchNorm1d(n_mel_channels),\n",
    "                nn.Dropout(0.5)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 멜 스펙트로그램 [batch_size, n_mel_channels, time]\n",
    "        Returns:\n",
    "            수정된 멜 스펙트로그램\n",
    "        \"\"\"\n",
    "        for i in range(len(self.convolutions) - 1):\n",
    "            x = self.convolutions[i](x)\n",
    "        x = self.convolutions[-1](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbdb327638f5d2e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:17.449719Z",
     "start_time": "2024-11-26T06:37:17.436813Z"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_embedding_dim, encoder_n_convolutions,\n",
    "                 encoder_kernel_size, encoder_lstm_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        convolutions = []\n",
    "        for _ in range(encoder_n_convolutions):\n",
    "            conv_layer = nn.Sequential(\n",
    "                nn.Conv1d(encoder_embedding_dim, encoder_embedding_dim,\n",
    "                         encoder_kernel_size, stride=1,\n",
    "                         padding=int((encoder_kernel_size - 1) / 2)),\n",
    "                nn.BatchNorm1d(encoder_embedding_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5)\n",
    "            )\n",
    "            convolutions.append(conv_layer)\n",
    "        self.convolutions = nn.ModuleList(convolutions)\n",
    "        \n",
    "        self.lstm = nn.LSTM(encoder_embedding_dim, encoder_lstm_dim,\n",
    "                           num_layers=1, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x, input_lengths):\n",
    "        \"\"\"\n",
    "        x: [B, embed_dim, T]\n",
    "        input_lengths: [B]\n",
    "        \"\"\"\n",
    "        # Conv layers\n",
    "        for conv in self.convolutions:\n",
    "            x = conv(x)\n",
    "        \n",
    "        # Prepare for LSTM\n",
    "        x = x.transpose(1, 2)  # [B, T, embed_dim]\n",
    "        \n",
    "        # Pack sequence\n",
    "        input_lengths = input_lengths.cpu()  # lengths를 CPU로 이동\n",
    "        \n",
    "        # Sort by length for packing\n",
    "        input_lengths, sort_idx = torch.sort(input_lengths, descending=True)\n",
    "        x = x[sort_idx]\n",
    "        \n",
    "        # Pack the sequence\n",
    "        x_packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, input_lengths.cpu(), batch_first=True)\n",
    "        \n",
    "        # LSTM forward\n",
    "        self.lstm.flatten_parameters()\n",
    "        outputs, _ = self.lstm(x_packed)\n",
    "        \n",
    "        # Unpack the sequence\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            outputs, batch_first=True)\n",
    "        \n",
    "        # Restore original order\n",
    "        _, unsort_idx = torch.sort(sort_idx)\n",
    "        outputs = outputs[unsort_idx]\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def inference(self, x):\n",
    "        for conv in self.convolutions:\n",
    "            x = conv(x)\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        self.lstm.flatten_parameters()\n",
    "        outputs, _ = self.lstm(x)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81e8237c2b5799d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:17.965286Z",
     "start_time": "2024-11-26T06:37:17.942282Z"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_mel_channels, encoder_embedding_dim,\n",
    "                 attention_dim, attention_location_n_filters,\n",
    "                 attention_location_kernel_size, attention_rnn_dim,\n",
    "                 decoder_rnn_dim, prenet_dim, max_decoder_steps,\n",
    "                 gate_threshold, p_attention_dropout, p_decoder_dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_mel_channels = n_mel_channels\n",
    "        self.encoder_embedding_dim = encoder_embedding_dim\n",
    "        self.attention_rnn_dim = attention_rnn_dim\n",
    "        self.decoder_rnn_dim = decoder_rnn_dim\n",
    "        self.prenet_dim = prenet_dim\n",
    "        self.max_decoder_steps = max_decoder_steps\n",
    "        self.gate_threshold = gate_threshold\n",
    "        self.p_attention_dropout = p_attention_dropout\n",
    "        self.p_decoder_dropout = p_decoder_dropout\n",
    "\n",
    "        # Prenet\n",
    "        self.prenet = Prenet(n_mel_channels, [prenet_dim, prenet_dim])\n",
    "\n",
    "        # Attention RNN\n",
    "        self.attention_rnn = nn.LSTMCell(\n",
    "            prenet_dim + encoder_embedding_dim,\n",
    "            attention_rnn_dim)\n",
    "\n",
    "        # Attention Layer\n",
    "        self.attention_layer = Attention(\n",
    "            attention_rnn_dim, encoder_embedding_dim,\n",
    "            attention_dim, attention_location_n_filters,\n",
    "            attention_location_kernel_size)\n",
    "\n",
    "        # Decoder RNN\n",
    "        self.decoder_rnn = nn.LSTMCell(\n",
    "            attention_rnn_dim + encoder_embedding_dim,\n",
    "            decoder_rnn_dim)\n",
    "\n",
    "        # Linear Projection\n",
    "        self.linear_projection = nn.Linear(\n",
    "            decoder_rnn_dim + encoder_embedding_dim,\n",
    "            n_mel_channels)  # 출력을 n_mel_channels로 수정\n",
    "\n",
    "        # Gate Layer\n",
    "        self.gate_layer = nn.Linear(\n",
    "            decoder_rnn_dim + encoder_embedding_dim, 1,\n",
    "            bias=True)\n",
    "\n",
    "        # Attention 관련 레이어 추가\n",
    "        self.attention_layer = Attention(\n",
    "            attention_rnn_dim,\n",
    "            encoder_embedding_dim,\n",
    "            attention_dim,\n",
    "            attention_location_n_filters,\n",
    "            attention_location_kernel_size\n",
    "        )\n",
    "        \n",
    "        # Memory layer 추가\n",
    "        self.memory_layer = nn.Linear(\n",
    "            encoder_embedding_dim,\n",
    "            attention_dim,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        # Attention context projection\n",
    "        self.attention_projection = nn.Linear(\n",
    "            encoder_embedding_dim,\n",
    "            decoder_rnn_dim,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "    def parse_decoder_inputs(self, decoder_inputs):\n",
    "        \"\"\" Prepares decoder inputs, i.e. mel outputs\n",
    "        Args:\n",
    "            decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
    "        \"\"\"\n",
    "        # (B, n_mel_channels, T) -> (B, T, n_mel_channels)\n",
    "        decoder_inputs = decoder_inputs.transpose(1, 2)\n",
    "        decoder_inputs = decoder_inputs.contiguous()\n",
    "        \n",
    "        # (B, T, n_mel_channels) -> (T, B, n_mel_channels)\n",
    "        decoder_inputs = decoder_inputs.transpose(0, 1)\n",
    "        return decoder_inputs\n",
    "\n",
    "    def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
    "        \"\"\" Prepares decoder outputs for output\n",
    "        Args:\n",
    "            mel_outputs: mel outputs from the decoder\n",
    "            gate_outputs: gate outputs from the decoder\n",
    "            alignments: alignments from the decoder\n",
    "        \"\"\"\n",
    "        # (T, B, n_mel_channels) -> (B, T, n_mel_channels)\n",
    "        mel_outputs = mel_outputs.transpose(0, 1).contiguous()\n",
    "        \n",
    "        # (T, B) -> (B, T)\n",
    "        gate_outputs = gate_outputs.transpose(0, 1).contiguous()\n",
    "        \n",
    "        # (T, B, n_text) -> (B, T, n_text)\n",
    "        alignments = alignments.transpose(0, 1).contiguous()\n",
    "\n",
    "        return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_inputs, memory_lengths=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_outputs: [batch_size, max_time, encoder_embedding_dim]\n",
    "            decoder_inputs: [batch_size, n_mel_channels, max_time]\n",
    "            memory_lengths: [batch_size]\n",
    "        \"\"\"\n",
    "        # decoder_inputs 형태 변환\n",
    "        decoder_inputs = decoder_inputs.transpose(1, 2)  # [batch_size, max_time, n_mel_channels]\n",
    "        \n",
    "        # 초기 상태 초기화\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        max_time = decoder_inputs.size(1)\n",
    "        \n",
    "        # 초기 attention context\n",
    "        attention_context = torch.zeros(\n",
    "            batch_size,\n",
    "            self.encoder_embedding_dim\n",
    "        ).to(encoder_outputs.device)\n",
    "        \n",
    "        # 초기 attention hidden states\n",
    "        attention_hidden = torch.zeros(\n",
    "            batch_size,\n",
    "            self.attention_rnn_dim\n",
    "        ).to(encoder_outputs.device)\n",
    "        \n",
    "        attention_cell = torch.zeros(\n",
    "            batch_size,\n",
    "            self.attention_rnn_dim\n",
    "        ).to(encoder_outputs.device)\n",
    "        \n",
    "        # 초기 decoder states\n",
    "        decoder_hidden = torch.zeros(\n",
    "            batch_size,\n",
    "            self.decoder_rnn_dim\n",
    "        ).to(encoder_outputs.device)\n",
    "        \n",
    "        decoder_cell = torch.zeros(\n",
    "            batch_size,\n",
    "            self.decoder_rnn_dim\n",
    "        ).to(encoder_outputs.device)\n",
    "        \n",
    "        # 초기 attention weights\n",
    "        attention_weights = torch.zeros(\n",
    "            batch_size,\n",
    "            encoder_outputs.size(1)\n",
    "        ).to(encoder_outputs.device)\n",
    "        \n",
    "        # 출력을 저장할 리스트\n",
    "        mel_outputs, gate_outputs, alignments = [], [], []\n",
    "        \n",
    "        # Memory를 미리 처리\n",
    "        processed_memory = self.memory_layer(encoder_outputs)\n",
    "        \n",
    "        # 각 타임스텝에 대해 처리\n",
    "        for i in range(max_time):\n",
    "            current_input = decoder_inputs[:, i, :]  # [batch_size, n_mel_channels]\n",
    "            current_input = self.prenet(current_input)  # [batch_size, prenet_dim]\n",
    "            \n",
    "            # Attention RNN\n",
    "            cell_input = torch.cat((current_input, attention_context), -1)\n",
    "            attention_hidden, attention_cell = self.attention_rnn(\n",
    "                cell_input, (attention_hidden, attention_cell))\n",
    "            attention_hidden = F.dropout(\n",
    "                attention_hidden, self.p_attention_dropout, self.training)\n",
    "            \n",
    "            # Attention 계산\n",
    "            attention_weights_cat = torch.cat(\n",
    "                (attention_weights.unsqueeze(1),\n",
    "                 attention_weights.unsqueeze(1)),\n",
    "                dim=1)\n",
    "            attention_context, attention_weights = self.attention_layer(\n",
    "                attention_hidden, encoder_outputs,\n",
    "                processed_memory, attention_weights_cat,\n",
    "                mask=None if memory_lengths is None else ~get_mask_from_lengths(memory_lengths))\n",
    "            \n",
    "            # Decoder RNN\n",
    "            decoder_input = torch.cat((attention_hidden, attention_context), -1)\n",
    "            decoder_hidden, decoder_cell = self.decoder_rnn(\n",
    "                decoder_input, (decoder_hidden, decoder_cell))\n",
    "            decoder_hidden = F.dropout(\n",
    "                decoder_hidden, self.p_decoder_dropout, self.training)\n",
    "            \n",
    "            # Linear projection\n",
    "            decoder_hidden_attention = torch.cat(\n",
    "                (decoder_hidden, attention_context), dim=1)\n",
    "            decoder_output = self.linear_projection(decoder_hidden_attention)\n",
    "            gate_prediction = self.gate_layer(decoder_hidden_attention)\n",
    "            \n",
    "            # 결과 저장\n",
    "            mel_outputs.append(decoder_output)\n",
    "            gate_outputs.append(gate_prediction)\n",
    "            alignments.append(attention_weights)\n",
    "        \n",
    "        # 리스트를 텐서로 변환\n",
    "        mel_outputs = torch.stack(mel_outputs)\n",
    "        gate_outputs = torch.stack(gate_outputs)\n",
    "        alignments = torch.stack(alignments)\n",
    "        \n",
    "        # 출력 형식 변환\n",
    "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "            mel_outputs, gate_outputs, alignments)\n",
    "        \n",
    "        return mel_outputs, gate_outputs, alignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "455fb4c9d54c8d4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:19.048799Z",
     "start_time": "2024-11-26T06:37:19.032971Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tacotron2(nn.Module):\n",
    "    attention_location_n_filters = 32\n",
    "    attention_location_kernel_size = 31\n",
    "    decoder_rnn_dim = 512\n",
    "    prenet_dim = 256\n",
    "    max_decoder_steps = 1000\n",
    "    gate_threshold = 0.5\n",
    "    p_attention_dropout = 0.1\n",
    "    p_decoder_dropout = 0.1\n",
    "    postnet_embedding_dim = 256\n",
    "    postnet_kernel_size = 5\n",
    "\n",
    "    def __init__(self, n_mel_channels, vocab_size, embedding_dim, \n",
    "                 encoder_n_convolutions, encoder_kernel_size,\n",
    "                 attention_rnn_dim, attention_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            encoder_embedding_dim=embedding_dim,  # 256\n",
    "            encoder_n_convolutions=encoder_n_convolutions,\n",
    "            encoder_kernel_size=encoder_kernel_size,\n",
    "            encoder_lstm_dim=int(embedding_dim/2)  # 128 (bi-directional = 256)\n",
    "        )\n",
    "        \n",
    "        self.decoder = Decoder(\n",
    "            n_mel_channels=n_mel_channels,\n",
    "            encoder_embedding_dim=embedding_dim,  # 256\n",
    "            attention_dim=attention_dim,  # 128\n",
    "            attention_location_n_filters=self.attention_location_n_filters,\n",
    "            attention_location_kernel_size=self.attention_location_kernel_size,\n",
    "            attention_rnn_dim=attention_rnn_dim,  # 512\n",
    "            decoder_rnn_dim=self.decoder_rnn_dim,  # 512\n",
    "            prenet_dim=self.prenet_dim,  # 128\n",
    "            max_decoder_steps=self.max_decoder_steps,\n",
    "            gate_threshold=self.gate_threshold,\n",
    "            p_attention_dropout=self.p_attention_dropout,\n",
    "            p_decoder_dropout=self.p_decoder_dropout\n",
    "        )\n",
    "        \n",
    "        self.postnet = Postnet(\n",
    "            n_mel_channels=n_mel_channels,\n",
    "            postnet_embedding_dim=self.postnet_embedding_dim,\n",
    "            postnet_kernel_size=self.postnet_kernel_size,\n",
    "            postnet_n_convolutions=5\n",
    "        )\n",
    "\n",
    "    def forward(self, text_inputs, text_lengths, mel_inputs, mel_lengths):\n",
    "        \"\"\"\n",
    "        텍스트를 멜 스펙트로그램으로 변환\n",
    "        \"\"\"\n",
    "        embedded_inputs = self.embedding(text_inputs).transpose(1, 2)\n",
    "        \n",
    "        encoder_outputs = self.encoder(embedded_inputs, text_lengths)\n",
    "        \n",
    "        mel_outputs, gate_outputs, alignments = self.decoder(\n",
    "            encoder_outputs, mel_inputs, text_lengths)\n",
    "        \n",
    "        # mel_outputs 차원 변환 (B, T, n_mel_channels) -> (B, n_mel_channels, T)\n",
    "        mel_outputs_transpose = mel_outputs.transpose(1, 2)\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs_transpose)\n",
    "        mel_outputs_postnet = mel_outputs_transpose + mel_outputs_postnet\n",
    "        \n",
    "        # 결과를 다시 원래 형태로 변환 (B, n_mel_channels, T) -> (B, T, n_mel_channels)\n",
    "        mel_outputs_postnet = mel_outputs_postnet.transpose(1, 2)\n",
    "        \n",
    "        return mel_outputs_postnet, mel_outputs, gate_outputs, alignments\n",
    "\n",
    "    def inference(self, text_inputs):\n",
    "        \"\"\"\n",
    "        추론 시 사용되는 forward 메서드\n",
    "        \"\"\"\n",
    "        embedded_inputs = self.embedding(text_inputs).transpose(1, 2)\n",
    "        encoder_outputs = self.encoder.inference(embedded_inputs)\n",
    "        mel_outputs, gate_outputs, alignments = self.decoder.inference(encoder_outputs)\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "        \n",
    "        return mel_outputs_postnet, mel_outputs, gate_outputs, alignments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77f284c8bd764f75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:20.988356Z",
     "start_time": "2024-11-26T06:37:20.981386Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_mask_from_lengths(lengths):\n",
    "    max_len = torch.max(lengths).item()\n",
    "    ids = torch.arange(0, max_len, device=lengths.device)\n",
    "    mask = (ids < lengths.unsqueeze(1)).bool()\n",
    "    return mask "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "85ca8a468aa3bedb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:38:32.569774Z",
     "start_time": "2024-11-26T06:38:32.563410Z"
    }
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "    \n",
    "config = Namespace(\n",
    "    device_num=4,\n",
    "    batch_size=4,\n",
    "    learning_rate=0.0002,\n",
    "    num_epochs=100,\n",
    "    start_epoch=0,\n",
    "    save_interval=10,\n",
    "    checkpoint_dir='checkpoints',\n",
    "    resume_checkpoint=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e389a0f54f7d3f46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:41:44.546176Z",
     "start_time": "2024-11-26T06:38:33.377727Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared_hdd/shared_hdd/haesol1013/anaconda3/envs/py311/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:134: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "  0%|          | 0/2892 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Error in train_step: tuple indices must be integers or slices, not str\n",
      "\n",
      "Batch information:\n",
      "Batch size: 6\n",
      "Batch contents: [<class 'torch.Tensor'>, <class 'torch.Tensor'>, <class 'torch.Tensor'>, <class 'torch.Tensor'>, <class 'torch.Tensor'>, <class 'torch.Tensor'>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(config)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m     \n",
      "Cell \u001b[0;32mIn[2], line 190\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    187\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader)\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[0;32m--> 190\u001b[0m     losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    193\u001b[0m     progress_bar\u001b[38;5;241m.\u001b[39mset_description(\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[2], line 172\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(batch)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch contents: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m[\u001b[38;5;28mtype\u001b[39m(x)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mx\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mbatch]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[2], line 100\u001b[0m, in \u001b[0;36mTrainer.train_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     99\u001b[0m         \u001b[38;5;66;03m# 배치 데이터 언패킹\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m         text_padded \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext_padded\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    101\u001b[0m         mel_padded \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmel_padded\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    102\u001b[0m         gate_padded \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgate_padded\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(config)\n",
    "trainer.train()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa54c83868a5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
