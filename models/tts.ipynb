{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:12.261698Z",
     "start_time": "2024-11-26T06:37:12.256401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from jamo import hangul_to_jamo\n",
    "import librosa\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm, spectral_norm\n",
    "import torchaudio\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm"
   ],
   "id": "4307d819d71e6b74",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:12.887202Z",
     "start_time": "2024-11-26T06:37:12.858049Z"
    }
   },
   "source": [
    "class Trainer:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.set_device(config.device_num)\n",
    "            self.device = torch.device(\"cuda\")\n",
    "        \n",
    "        # 데이터셋 및 데이터로더 초기화\n",
    "        self.train_dataset = KSSTTSDataset(split='train')\n",
    "        self.valid_dataset = KSSTTSDataset(split='valid')\n",
    "        \n",
    "        self.train_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=KSSTTSDataset.collate_fn,\n",
    "            num_workers=4\n",
    "        )\n",
    "        \n",
    "        self.valid_loader = DataLoader(\n",
    "            self.valid_dataset,\n",
    "            batch_size=config.batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=KSSTTSDataset.collate_fn,\n",
    "            num_workers=4\n",
    "        )\n",
    "        \n",
    "        # 모델 초기화\n",
    "        self.tacotron2 = Tacotron2(\n",
    "            n_mel_channels=80,\n",
    "            vocab_size=len(self.train_dataset.vocab),\n",
    "            embedding_dim=256,\n",
    "            encoder_n_convolutions=3,\n",
    "            encoder_kernel_size=5,\n",
    "            attention_rnn_dim=512,\n",
    "            attention_dim=128\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.generator = Generator().to(self.device)\n",
    "        self.msd = MultiScaleDiscriminator().to(self.device)\n",
    "        self.mpd = MultiPeriodDiscriminator().to(self.device)\n",
    "        self.mrf = MRFDiscriminator().to(self.device)\n",
    "        \n",
    "        # Loss 함수 초기화\n",
    "        self.criterion = TTSLoss(device=self.device)\n",
    "        \n",
    "        # Optimizer 초기화\n",
    "        self.optimizer_g = optim.AdamW([\n",
    "            {'params': self.tacotron2.parameters()},\n",
    "            {'params': self.generator.parameters()}\n",
    "        ], lr=config.learning_rate, betas=(0.8, 0.99), weight_decay=0.01)\n",
    "        \n",
    "        self.optimizer_d = optim.AdamW([\n",
    "            {'params': self.msd.parameters()},\n",
    "            {'params': self.mpd.parameters()},\n",
    "            {'params': self.mrf.parameters()}\n",
    "        ], lr=config.learning_rate, betas=(0.8, 0.99), weight_decay=0.01)\n",
    "        \n",
    "        # 체크포인트 디렉토리 생성\n",
    "        self.checkpoint_dir = Path(config.checkpoint_dir)\n",
    "        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def save_checkpoint(self, epoch, loss):\n",
    "        checkpoint = {\n",
    "            'epoch': epoch,\n",
    "            'tacotron2_state_dict': self.tacotron2.state_dict(),\n",
    "            'generator_state_dict': self.generator.state_dict(),\n",
    "            'msd_state_dict': self.msd.state_dict(),\n",
    "            'mpd_state_dict': self.mpd.state_dict(),\n",
    "            'mrf_state_dict': self.mrf.state_dict(),\n",
    "            'optimizer_t2_state_dict': self.optimizer_t2.state_dict(),\n",
    "            'optimizer_g_state_dict': self.optimizer_g.state_dict(),\n",
    "            'optimizer_d_state_dict': self.optimizer_d.state_dict(),\n",
    "            'loss': loss\n",
    "        }\n",
    "        \n",
    "        path = self.checkpoint_dir / f'checkpoint_epoch_{epoch}.pt'\n",
    "        torch.save(checkpoint, path)\n",
    "\n",
    "    def load_checkpoint(self, checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        \n",
    "        self.tacotron2.load_state_dict(checkpoint['tacotron2_state_dict'])\n",
    "        self.generator.load_state_dict(checkpoint['generator_state_dict'])\n",
    "        self.msd.load_state_dict(checkpoint['msd_state_dict'])\n",
    "        self.mpd.load_state_dict(checkpoint['mpd_state_dict'])\n",
    "        self.mrf.load_state_dict(checkpoint['mrf_state_dict'])\n",
    "        \n",
    "        self.optimizer_t2.load_state_dict(checkpoint['optimizer_t2_state_dict'])\n",
    "        self.optimizer_g.load_state_dict(checkpoint['optimizer_g_state_dict'])\n",
    "        self.optimizer_d.load_state_dict(checkpoint['optimizer_d_state_dict'])\n",
    "        \n",
    "        return checkpoint['epoch']\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        try:\n",
    "            # 배치 데이터 언패킹\n",
    "            text_padded = batch['text_padded'].to(self.device)\n",
    "            mel_padded = batch['mel_padded'].to(self.device)\n",
    "            gate_padded = batch['gate_padded'].to(self.device)\n",
    "            audio_padded = batch['audio_padded'].to(self.device)\n",
    "            text_lengths = batch['text_lengths'].to(self.device)\n",
    "            mel_lengths = batch['mel_lengths'].to(self.device)\n",
    "            \n",
    "            # 디버깅을 위한 shape 출력\n",
    "            print(\"\\nInput shapes:\")\n",
    "            print(f\"text_padded: {text_padded.shape}\")\n",
    "            print(f\"mel_padded: {mel_padded.shape}\")\n",
    "            print(f\"gate_padded: {gate_padded.shape}\")\n",
    "            print(f\"audio_padded: {audio_padded.shape}\")\n",
    "            \n",
    "            # Tacotron2 forward\n",
    "            mel_outputs_postnet, mel_outputs, gate_outputs, _ = self.tacotron2(\n",
    "                text_padded, text_lengths, mel_padded, mel_lengths)\n",
    "            \n",
    "            # Generator forward\n",
    "            mel_for_generator = mel_outputs_postnet.transpose(1, 2)\n",
    "            fake_audio = self.generator(mel_for_generator)\n",
    "            \n",
    "            # Discriminator forward\n",
    "            msd_real_outputs, msd_real_features = self.msd(audio_padded)\n",
    "            msd_fake_outputs, msd_fake_features = self.msd(fake_audio.detach())\n",
    "            \n",
    "            mpd_real_outputs, mpd_real_features = self.mpd(audio_padded)\n",
    "            mpd_fake_outputs, mpd_fake_features = self.mpd(fake_audio.detach())\n",
    "            \n",
    "            # Loss 계산\n",
    "            # Tacotron2 loss\n",
    "            tacotron2_losses = self.criterion.tacotron2_loss(\n",
    "                mel_outputs, mel_outputs_postnet, gate_outputs,\n",
    "                mel_padded, gate_padded, mel_lengths\n",
    "            )\n",
    "            \n",
    "            # HiFi-GAN loss\n",
    "            real_outputs = msd_real_outputs + mpd_real_outputs\n",
    "            fake_outputs = msd_fake_outputs + mpd_fake_outputs\n",
    "            real_features = msd_real_features + mpd_real_features\n",
    "            fake_features = msd_fake_features + mpd_fake_features\n",
    "            \n",
    "            hifigan_losses = self.criterion.hifi_gan_loss(\n",
    "                audio_padded, fake_audio,\n",
    "                real_outputs, fake_outputs,\n",
    "                real_features, fake_features\n",
    "            )\n",
    "            \n",
    "            # Optimizer step\n",
    "            # Generator update\n",
    "            self.optimizer_g.zero_grad()\n",
    "            g_loss = tacotron2_losses['total_loss'] + hifigan_losses['generator_loss']\n",
    "            g_loss.backward(retain_graph=True)\n",
    "            self.optimizer_g.step()\n",
    "            \n",
    "            # Discriminator update\n",
    "            self.optimizer_d.zero_grad()\n",
    "            d_loss = hifigan_losses['discriminator_loss']\n",
    "            d_loss.backward()\n",
    "            self.optimizer_d.step()\n",
    "            \n",
    "            return {\n",
    "                'total_loss': g_loss + d_loss,\n",
    "                **tacotron2_losses,\n",
    "                **hifigan_losses\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError in train_step: {str(e)}\")\n",
    "            print(\"\\nBatch information:\")\n",
    "            print(f\"Batch size: {len(batch)}\")\n",
    "            print(f\"Batch contents: {[type(x) for x in batch]}\")\n",
    "            raise e\n",
    "\n",
    "    def train(self):\n",
    "        start_epoch = 0\n",
    "        if self.config.resume_checkpoint:\n",
    "            start_epoch = self.load_checkpoint(self.config.resume_checkpoint)\n",
    "        \n",
    "        for epoch in range(start_epoch, self.config.num_epochs):\n",
    "            self.tacotron2.train()\n",
    "            self.generator.train()\n",
    "            self.msd.train()\n",
    "            self.mpd.train()\n",
    "            self.mrf.train()\n",
    "            \n",
    "            total_loss = 0\n",
    "            progress_bar = tqdm(self.train_loader)\n",
    "            \n",
    "            for batch in progress_bar:\n",
    "                losses = self.train_step(batch)\n",
    "                total_loss += losses['total_loss'].item()\n",
    "                \n",
    "                progress_bar.set_description(\n",
    "                    f\"Epoch {epoch+1}, Loss: {losses['total_loss'].item():.4f}\"\n",
    "                )\n",
    "            \n",
    "            avg_loss = total_loss / len(self.train_loader)\n",
    "            \n",
    "            if (epoch + 1) % self.config.save_interval == 0:\n",
    "                self.save_checkpoint(epoch + 1, avg_loss)\n",
    "            \n",
    "            print(f'Epoch {epoch+1} completed. Average Loss: {avg_loss:.4f}')"
   ],
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:13.291211Z",
     "start_time": "2024-11-26T06:37:13.268245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class KSSTTSDataset(Dataset):\n",
    "    def __init__(self, split='train', valid_size=0.1, seed=42, target_sr=22050):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 기본 설정\n",
    "        self.target_sr = target_sr\n",
    "        self.vocab = None\n",
    "        \n",
    "        # 데이터셋 로드\n",
    "        dataset = load_dataset(\"Bingsu/KSS_Dataset\")\n",
    "        full_dataset = dataset['train']\n",
    "        \n",
    "        # train/valid 분할\n",
    "        train_idx, valid_idx = train_test_split(\n",
    "            range(len(full_dataset)),\n",
    "            test_size=valid_size,\n",
    "            random_state=seed\n",
    "        )\n",
    "        \n",
    "        self.indices = train_idx if split == 'train' else valid_idx\n",
    "        self.dataset = full_dataset\n",
    "        \n",
    "        # vocab 초기화\n",
    "        self._initialize_vocab()\n",
    "        \n",
    "    def _initialize_vocab(self):\n",
    "        all_texts = [self.dataset[idx]['original_script'] for idx in self.indices]\n",
    "        unique_tokens = set()\n",
    "        \n",
    "        for text in all_texts:\n",
    "            sequence = self._text_to_sequence(text)\n",
    "            unique_tokens.update(sequence)\n",
    "            \n",
    "        self.vocab = {token: idx for idx, token in enumerate(sorted(unique_tokens))}\n",
    "        self.vocab['<pad>'] = len(self.vocab)\n",
    "        self.vocab['<unk>'] = len(self.vocab)\n",
    "    \n",
    "    def _text_to_sequence(self, text):\n",
    "        sequence = []\n",
    "        for char in text:\n",
    "            if '가' <= char <= '힣':\n",
    "                jamos = list(hangul_to_jamo(char))\n",
    "                sequence.extend(jamos)\n",
    "            else:\n",
    "                sequence.append(char)\n",
    "        return sequence\n",
    "    \n",
    "    def _wav_to_mel(self, wav):\n",
    "        stft = librosa.stft(wav, n_fft=1024, hop_length=256, win_length=1024)\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            S=np.abs(stft)**2,\n",
    "            sr=self.target_sr,\n",
    "            n_mels=80,\n",
    "            fmin=0,\n",
    "            fmax=8000\n",
    "        )\n",
    "        mel_spec = np.log(np.clip(mel_spec, a_min=1e-5, a_max=None))\n",
    "        mel_spec = np.clip(mel_spec, a_min=-4, a_max=4)\n",
    "        mel_spec = (mel_spec + 4) / 8\n",
    "        return mel_spec\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        real_idx = self.indices[idx]\n",
    "        item = self.dataset[real_idx]\n",
    "        \n",
    "        # 텍스트 처리\n",
    "        text = item['original_script']\n",
    "        sequence = self._text_to_sequence(text)\n",
    "        sequence = [self.vocab.get(char, self.vocab['<unk>']) for char in sequence]\n",
    "        sequence = torch.LongTensor(sequence)\n",
    "        \n",
    "        # 오디오 처리\n",
    "        audio = torch.from_numpy(item['audio']['array']).float()\n",
    "        original_sr = item['audio']['sampling_rate']\n",
    "        \n",
    "        if original_sr != self.target_sr:\n",
    "            resampler = torchaudio.transforms.Resample(\n",
    "                orig_freq=original_sr,\n",
    "                new_freq=self.target_sr\n",
    "            )\n",
    "            audio = resampler(audio)\n",
    "        \n",
    "        # 멜 스펙트로그램 변환\n",
    "        mel = self._wav_to_mel(audio.numpy())\n",
    "        mel = torch.FloatTensor(mel).transpose(0, 1)\n",
    "        \n",
    "        return {\n",
    "            'text': sequence,\n",
    "            'text_length': sequence.size(0),\n",
    "            'mel': mel,\n",
    "            'mel_length': mel.size(0),\n",
    "            'audio': audio\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_fn(batch):\n",
    "        # 텍스트 패딩\n",
    "        text_lengths = [x['text'].size(0) for x in batch]\n",
    "        max_text_len = max(text_lengths)\n",
    "        text_padded = torch.zeros(len(batch), max_text_len, dtype=torch.long)\n",
    "        for i, x in enumerate(batch):\n",
    "            text = x['text']\n",
    "            text_padded[i, :len(text)] = text\n",
    "        \n",
    "        # 멜 스펙트로그램 패딩\n",
    "        mel_lengths = [x['mel'].size(0) for x in batch]\n",
    "        max_mel_len = max(mel_lengths)\n",
    "        mel_padded = torch.zeros(len(batch), 80, max_mel_len)\n",
    "        for i, x in enumerate(batch):\n",
    "            mel = x['mel']\n",
    "            mel_padded[i, :, :mel.size(0)] = mel.transpose(0, 1)\n",
    "        \n",
    "        # 오디오 패딩\n",
    "        audio_lengths = [x['audio'].size(0) for x in batch]\n",
    "        max_audio_len = max(audio_lengths)\n",
    "        audio_padded = torch.zeros(len(batch), 1, max_audio_len)\n",
    "        for i, x in enumerate(batch):\n",
    "            audio = x['audio']\n",
    "            audio_padded[i, 0, :audio.size(0)] = audio\n",
    "        \n",
    "        # gate 패딩 생성\n",
    "        gate_padded = torch.zeros(len(batch), max_mel_len)\n",
    "        for i, length in enumerate(mel_lengths):\n",
    "            gate_padded[i, length-1:] = 1\n",
    "        \n",
    "        # 길이 정보도 함께 반환\n",
    "        text_lengths = torch.LongTensor(text_lengths)\n",
    "        mel_lengths = torch.LongTensor(mel_lengths)\n",
    "        \n",
    "        return {\n",
    "            'text_padded': text_padded,\n",
    "            'mel_padded': mel_padded,\n",
    "            'gate_padded': gate_padded,\n",
    "            'audio_padded': audio_padded,\n",
    "            'text_lengths': text_lengths,\n",
    "            'mel_lengths': mel_lengths\n",
    "        }"
   ],
   "id": "bb0e5859b3e6d1fe",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:13.590133Z",
     "start_time": "2024-11-26T06:37:13.581889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1, negative_slope=0.01):\n",
    "        super().__init__()\n",
    "        # 패딩 계산을 동적으로 수행\n",
    "        self.padding = (dilation * (kernel_size - 1)) // 2\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels, out_channels, kernel_size,\n",
    "            stride=stride, padding=self.padding, dilation=dilation\n",
    "        )\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            out_channels, out_channels, kernel_size,\n",
    "            stride=stride, padding=self.padding, dilation=dilation\n",
    "        )\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope=negative_slope)\n",
    "        self.skip_connection = (\n",
    "            nn.Conv1d(in_channels, out_channels, 1)\n",
    "            if in_channels != out_channels else\n",
    "            nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = self.skip_connection(x)\n",
    "        x = self.leaky_relu(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        # 차원이 다른 경우 residual을 조정\n",
    "        if x.size(-1) != residual.size(-1):\n",
    "            # 더 작은 크기에 맞춤\n",
    "            target_size = min(x.size(-1), residual.size(-1))\n",
    "            x = x[..., :target_size]\n",
    "            residual = residual[..., :target_size]\n",
    "            \n",
    "        return self.leaky_relu(x + residual)"
   ],
   "id": "7639d50c4afaf64a",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:13.835611Z",
     "start_time": "2024-11-26T06:37:13.820817Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_size=80):\n",
    "        super().__init__()\n",
    "        # Initial conv\n",
    "        self.conv_pre = nn.Conv1d(input_size, 512, 7, 1, 3)\n",
    "        \n",
    "        # Upsampling layers with kernel size and stride adjustments\n",
    "        self.ups = nn.ModuleList([\n",
    "            nn.ConvTranspose1d(512, 256, 16, 8, 4),\n",
    "            nn.ConvTranspose1d(256, 128, 16, 8, 4),\n",
    "            nn.ConvTranspose1d(128, 64, 4, 2, 1),\n",
    "            nn.ConvTranspose1d(64, 32, 4, 2, 1),\n",
    "        ])\n",
    "        \n",
    "        # Multi-Receptive Field Fusion\n",
    "        self.mrf_blocks = nn.ModuleList([\n",
    "            # First MRF block\n",
    "            nn.ModuleList([\n",
    "                ResBlock(256, 256, kernel_size=3, dilation=1),\n",
    "                ResBlock(256, 256, kernel_size=3, dilation=3),\n",
    "                ResBlock(256, 256, kernel_size=3, dilation=5)\n",
    "            ]),\n",
    "            # Second MRF block\n",
    "            nn.ModuleList([\n",
    "                ResBlock(128, 128, kernel_size=3, dilation=1),\n",
    "                ResBlock(128, 128, kernel_size=3, dilation=3),\n",
    "                ResBlock(128, 128, kernel_size=3, dilation=5)\n",
    "            ]),\n",
    "            # Third MRF block\n",
    "            nn.ModuleList([\n",
    "                ResBlock(64, 64, kernel_size=3, dilation=1),\n",
    "                ResBlock(64, 64, kernel_size=3, dilation=3)\n",
    "            ]),\n",
    "            # Fourth MRF block\n",
    "            nn.ModuleList([\n",
    "                ResBlock(32, 32, kernel_size=3, dilation=1),\n",
    "                ResBlock(32, 32, kernel_size=3, dilation=2)\n",
    "            ])\n",
    "        ])\n",
    "        \n",
    "        # Final conv\n",
    "        self.conv_post = nn.Conv1d(32, 1, 7, 1, 3)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"Generator input shape: {x.shape}\")\n",
    "        x = self.conv_pre(x)\n",
    "        print(f\"After conv_pre: {x.shape}\")\n",
    "        \n",
    "        for i in range(len(self.ups)):\n",
    "            x = self.leaky_relu(x)\n",
    "            x = self.ups[i](x)\n",
    "            print(f\"After up {i}: {x.shape}\")\n",
    "            \n",
    "            # Apply MRF blocks\n",
    "            xs = None\n",
    "            for j, resblock in enumerate(self.mrf_blocks[i]):\n",
    "                if xs is None:\n",
    "                    xs = resblock(x)\n",
    "                else:\n",
    "                    # 크기가 다른 경우 처리\n",
    "                    res_out = resblock(x)\n",
    "                    if res_out.size(-1) != xs.size(-1):\n",
    "                        target_size = min(res_out.size(-1), xs.size(-1))\n",
    "                        xs = xs[..., :target_size]\n",
    "                        res_out = res_out[..., :target_size]\n",
    "                    xs += res_out\n",
    "            x = xs / len(self.mrf_blocks[i])\n",
    "        \n",
    "        print(f\"Generator output shape: {x.shape}\")\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.conv_post(x)\n",
    "        x = torch.tanh(x)\n",
    "        \n",
    "        return x"
   ],
   "id": "d3dcaac7ab138c26",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:14.043329Z",
     "start_time": "2024-11-26T06:37:14.037012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiScaleDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.discriminators = nn.ModuleList([\n",
    "            DiscriminatorS(use_spectral_norm=True),\n",
    "            DiscriminatorS(),\n",
    "            DiscriminatorS(),\n",
    "        ])\n",
    "        self.meanpools = nn.ModuleList([\n",
    "            nn.AvgPool1d(4, 2, padding=2),\n",
    "            nn.AvgPool1d(4, 2, padding=2)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: 입력 오디오 (fake 또는 real)\n",
    "        \"\"\"\n",
    "        y_d_rs = []  # discriminator outputs\n",
    "        fmap_rs = []  # feature maps\n",
    "\n",
    "        for i, d in enumerate(self.discriminators):\n",
    "            if i != 0:\n",
    "                x = self.meanpools[i-1](x)\n",
    "            y_d_r, fmap_r = d(x)\n",
    "            y_d_rs.append(y_d_r)\n",
    "            fmap_rs.append(fmap_r)\n",
    "\n",
    "        return y_d_rs, fmap_rs"
   ],
   "id": "d5c5d1fdb30962d",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:14.258300Z",
     "start_time": "2024-11-26T06:37:14.249974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DiscriminatorS(nn.Module):\n",
    "    def __init__(self, use_spectral_norm=False):\n",
    "        super().__init__()\n",
    "        norm_f = weight_norm if not use_spectral_norm else spectral_norm\n",
    "        self.convs = nn.ModuleList([\n",
    "            norm_f(nn.Conv1d(1, 128, 15, 1, padding=7)),\n",
    "            norm_f(nn.Conv1d(128, 128, 41, 2, groups=4, padding=20)),\n",
    "            norm_f(nn.Conv1d(128, 256, 41, 2, groups=16, padding=20)),\n",
    "            norm_f(nn.Conv1d(256, 512, 41, 4, groups=16, padding=20)),\n",
    "            norm_f(nn.Conv1d(512, 1024, 41, 4, groups=16, padding=20)),\n",
    "            norm_f(nn.Conv1d(1024, 1024, 41, 1, groups=16, padding=20)),\n",
    "            norm_f(nn.Conv1d(1024, 1024, 5, 1, padding=2)),\n",
    "        ])\n",
    "        self.conv_post = norm_f(nn.Conv1d(1024, 1, 3, 1, padding=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        fmap = []\n",
    "        for l in self.convs:\n",
    "            x = l(x)\n",
    "            x = F.leaky_relu(x, 0.1)\n",
    "            fmap.append(x)\n",
    "        x = self.conv_post(x)\n",
    "        fmap.append(x)\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "        return x, fmap"
   ],
   "id": "82a670b22e6ab8f7",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:14.486401Z",
     "start_time": "2024-11-26T06:37:14.476736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiPeriodDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.discriminators = nn.ModuleList([\n",
    "            DiscriminatorP(2),\n",
    "            DiscriminatorP(3),\n",
    "            DiscriminatorP(5),\n",
    "            DiscriminatorP(7),\n",
    "            DiscriminatorP(11),\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: 입력 오디오 (fake 또는 real)\n",
    "        \"\"\"\n",
    "        y_d_rs = []  # discriminator outputs\n",
    "        fmap_rs = []  # feature maps\n",
    "\n",
    "        for d in self.discriminators:\n",
    "            y_d_r, fmap_r = d(x)\n",
    "            y_d_rs.append(y_d_r)\n",
    "            fmap_rs.append(fmap_r)\n",
    "\n",
    "        return y_d_rs, fmap_rs"
   ],
   "id": "d25351488311bb23",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:14.827720Z",
     "start_time": "2024-11-26T06:37:14.816894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DiscriminatorP(nn.Module):\n",
    "    def __init__(self, period):\n",
    "        super().__init__()\n",
    "        self.period = period\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            weight_norm(nn.Conv2d(1, 32, (5, 1), (3, 1), padding=(2, 0))),\n",
    "            weight_norm(nn.Conv2d(32, 128, (5, 1), (3, 1), padding=(2, 0))),\n",
    "            weight_norm(nn.Conv2d(128, 512, (5, 1), (3, 1), padding=(2, 0))),\n",
    "            weight_norm(nn.Conv2d(512, 1024, (5, 1), (3, 1), padding=(2, 0))),\n",
    "            weight_norm(nn.Conv2d(1024, 1024, (5, 1), 1, padding=(2, 0))),\n",
    "        ])\n",
    "        self.conv_post = weight_norm(nn.Conv2d(1024, 1, (3, 1), 1, padding=(1, 0)))\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, 1, T]\n",
    "        \"\"\"\n",
    "        fmap = []\n",
    "        \n",
    "        # 1D -> 2D\n",
    "        b, c, t = x.shape\n",
    "        if t % self.period != 0:  # 패딩 추가\n",
    "            n_pad = self.period - (t % self.period)\n",
    "            x = F.pad(x, (0, n_pad), \"reflect\")\n",
    "            t = t + n_pad\n",
    "        x = x.view(b, c, t // self.period, self.period)\n",
    "\n",
    "        for layer in self.convs:\n",
    "            x = layer(x)\n",
    "            x = self.leaky_relu(x)\n",
    "            fmap.append(x)\n",
    "        x = self.conv_post(x)\n",
    "        fmap.append(x)\n",
    "        x = torch.flatten(x, 1, -1)\n",
    "\n",
    "        return x, fmap"
   ],
   "id": "157c8527b4139627",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:15.049102Z",
     "start_time": "2024-11-26T06:37:15.038213Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MRFDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # 초기 채널 조정을 위한 Conv1d 레이어들\n",
    "        self.msd_adjust = nn.Conv1d(1, 64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # MPD feature를 위한 2D -> 1D 변환 레이어\n",
    "        self.mpd_adjust = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(32, 64, kernel_size=(3, 3), padding=(1, 1)),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        \n",
    "        # 공통 처리를 위한 레이어들\n",
    "        self.shared_conv = nn.Sequential(\n",
    "            nn.Conv1d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv1d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv1d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.LeakyReLU(0.1)\n",
    "        )\n",
    "        \n",
    "        # 최종 출력을 위한 레이어\n",
    "        self.output_layer = nn.Conv1d(512, 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, msd_features, mpd_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            msd_features: List[List[Tensor]] - MultiScaleDiscriminator의 feature maps\n",
    "            mpd_features: List[List[Tensor]] - MultiPeriodDiscriminator의 feature maps\n",
    "        \"\"\"\n",
    "        # 마지막 레이어의 feature map 사용\n",
    "        msd_feat = msd_features[-1][-1]  # [B, 1, T]\n",
    "        mpd_feat = mpd_features[-1][-1]  # [B, 1, H, W]\n",
    "        \n",
    "        # MPD feature 처리\n",
    "        if mpd_feat.dim() == 4:\n",
    "            B, C, H, W = mpd_feat.size()\n",
    "            # 2D 처리\n",
    "            mpd_processed = self.mpd_adjust(mpd_feat)  # [B, 64, H, W]\n",
    "            # Flatten H, W 차원\n",
    "            mpd_processed = mpd_processed.view(B, 64, -1)  # [B, 64, H*W]\n",
    "        \n",
    "        # MSD feature 처리\n",
    "        msd_processed = self.msd_adjust(msd_feat)  # [B, 64, T]\n",
    "        \n",
    "        # 시간 차원 맞추기\n",
    "        target_length = min(msd_processed.size(-1), mpd_processed.size(-1))\n",
    "        msd_processed = F.interpolate(msd_processed, size=target_length, mode='linear', align_corners=False)\n",
    "        mpd_processed = F.interpolate(mpd_processed, size=target_length, mode='linear', align_corners=False)\n",
    "        \n",
    "        # Feature map 결합\n",
    "        combined = torch.cat([msd_processed, mpd_processed], dim=1)  # [B, 128, T]\n",
    "        \n",
    "        # 공통 처리\n",
    "        x = self.shared_conv(combined)\n",
    "        \n",
    "        # 최종 출력\n",
    "        output = self.output_layer(x)\n",
    "        \n",
    "        return output"
   ],
   "id": "1653e2b27914b023",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:15.239217Z",
     "start_time": "2024-11-26T06:37:15.217785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TTSLoss(nn.Module):\n",
    "    def __init__(self, device='cuda'):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "        \n",
    "        # Loss weights from the paper\n",
    "        self.lambda_mel = 45.0\n",
    "        self.lambda_fm = 2.0\n",
    "        self.lambda_adv = 1.0\n",
    "        \n",
    "        # Mel-spectrogram transform configuration\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=22050,\n",
    "            n_fft=1024,\n",
    "            win_length=1024,\n",
    "            hop_length=256,\n",
    "            f_min=0,\n",
    "            f_max=8000,\n",
    "            n_mels=80,\n",
    "            power=1.0,\n",
    "            normalized=True\n",
    "        ).to(device)\n",
    "\n",
    "    def tacotron2_loss(self, mel_output, mel_output_postnet, gate_out, \n",
    "                      mel_target, gate_target, mel_lengths):\n",
    "        \"\"\"Tacotron2 Loss Calculation\"\"\"\n",
    "        # mel_target: [B, n_mel_channels, T]\n",
    "        # mel_output: [B, T, n_mel_channels]\n",
    "        \n",
    "        # 차원 맞추기\n",
    "        mel_target = mel_target.transpose(1, 2)  # [B, T, n_mel_channels]\n",
    "        \n",
    "        # gate_out 차원 조정 [B, T, 1] -> [B, T]\n",
    "        gate_out = gate_out.squeeze(-1)\n",
    "        \n",
    "        # 텐서 차원 확인\n",
    "        B, T, C = mel_target.size()  # [batch_size, time_steps, n_mel_channels]\n",
    "        \n",
    "        # 마스크 생성 (B, T)\n",
    "        mask = ~self.get_mask_from_lengths(mel_lengths)\n",
    "        \n",
    "        # 마스크를 멜 스펙트로그램 차원에 맞게 조정\n",
    "        mel_mask = mask.unsqueeze(-1).expand(-1, -1, C)  # [B, T, C]\n",
    "        \n",
    "        # 마스킹 적용\n",
    "        mel_target_masked = mel_target.masked_fill(mel_mask, 0)\n",
    "        mel_output_masked = mel_output.masked_fill(mel_mask, 0)\n",
    "        mel_output_postnet_masked = mel_output_postnet.masked_fill(mel_mask, 0)\n",
    "        \n",
    "        # gate에 대한 마스킹 적용\n",
    "        gate_target = gate_target.masked_fill(mask, 0)\n",
    "        gate_out = gate_out.masked_fill(mask, 0)\n",
    "        \n",
    "        # Loss 계산\n",
    "        mel_loss = self.l1_loss(mel_output_masked, mel_target_masked) + \\\n",
    "                  self.l1_loss(mel_output_postnet_masked, mel_target_masked)\n",
    "        gate_loss = self.bce_loss(gate_out, gate_target)\n",
    "        \n",
    "        return {\n",
    "            'mel_loss': mel_loss,\n",
    "            'gate_loss': gate_loss,\n",
    "            'total_loss': mel_loss + gate_loss\n",
    "        }\n",
    "\n",
    "    def mel_spectrogram_loss(self, real_wave, fake_wave):\n",
    "        \"\"\"Mel-spectrogram L1 Loss\"\"\"\n",
    "        # 더 작은 길이에 맞추기\n",
    "        min_length = min(real_wave.size(-1), fake_wave.size(-1))\n",
    "        real_wave = real_wave[..., :min_length]\n",
    "        fake_wave = fake_wave[..., :min_length]\n",
    "        \n",
    "        # Mel spectrogram 생성\n",
    "        real_mel = self.mel_transform(real_wave)\n",
    "        fake_mel = self.mel_transform(fake_wave)\n",
    "        \n",
    "        # 디버깅을 위한 shape 출력\n",
    "        print(f\"Real wave shape: {real_wave.shape}\")\n",
    "        print(f\"Fake wave shape: {fake_wave.shape}\")\n",
    "        print(f\"Real mel shape: {real_mel.shape}\")\n",
    "        print(f\"Fake mel shape: {fake_mel.shape}\")\n",
    "        \n",
    "        return self.l1_loss(fake_mel, real_mel) * self.lambda_mel\n",
    "\n",
    "    def feature_matching_loss(self, fmap_r, fmap_g):\n",
    "        loss = 0\n",
    "        for dr, dg in zip(fmap_r, fmap_g):\n",
    "            for rl, gl in zip(dr, dg):\n",
    "                if isinstance(rl, torch.Tensor) and isinstance(gl, torch.Tensor):\n",
    "                    # 더 작은 크기에 맞추기\n",
    "                    if rl.dim() == gl.dim():\n",
    "                        min_length = min(rl.size(-1), gl.size(-1))\n",
    "                        if rl.dim() == 3:  # MSD features\n",
    "                            rl = rl[..., :min_length]\n",
    "                            gl = gl[..., :min_length]\n",
    "                        elif rl.dim() == 4:  # MPD features\n",
    "                            rl = rl[..., :min_length, :]\n",
    "                            gl = gl[..., :min_length, :]\n",
    "                        \n",
    "                        loss += torch.mean(torch.abs(rl - gl))\n",
    "        \n",
    "        return loss * self.lambda_fm\n",
    "\n",
    "    def generator_loss(self, disc_outputs):\n",
    "        loss = 0\n",
    "        for dg in disc_outputs:\n",
    "            loss += torch.mean((1-dg)**2)\n",
    "        return loss\n",
    "\n",
    "    def discriminator_loss(self, disc_real_outputs, disc_generated_outputs):\n",
    "        loss = 0\n",
    "        for dr, dg in zip(disc_real_outputs, disc_generated_outputs):\n",
    "            r_loss = torch.mean((1-dr)**2)\n",
    "            g_loss = torch.mean(dg**2)\n",
    "            loss += (r_loss + g_loss)\n",
    "        return loss\n",
    "\n",
    "    def hifi_gan_loss(self, real_wave, fake_wave, real_outputs, fake_outputs, real_feats, fake_feats):\n",
    "        \"\"\"HiFi-GAN Total Loss Calculation\"\"\"\n",
    "        # 차원 확인 및 조정\n",
    "        if real_wave.dim() == 2:\n",
    "            real_wave = real_wave.unsqueeze(1)\n",
    "        if fake_wave.dim() == 2:\n",
    "            fake_wave = fake_wave.unsqueeze(1)\n",
    "        \n",
    "        # 길이 맞추기\n",
    "        min_length = min(real_wave.size(-1), fake_wave.size(-1))\n",
    "        real_wave = real_wave[..., :min_length]\n",
    "        fake_wave = fake_wave[..., :min_length]\n",
    "        \n",
    "        # 디버깅을 위한 shape 출력\n",
    "        print(f\"\\nFeature matching shapes:\")\n",
    "        print(f\"Real features length: {[[(f.shape) for f in disc] for disc in real_feats]}\")\n",
    "        print(f\"Fake features length: {[[(f.shape) for f in disc] for disc in fake_feats]}\")\n",
    "        \n",
    "        # Loss 계산\n",
    "        mel_loss = self.mel_spectrogram_loss(real_wave, fake_wave)\n",
    "        fm_loss = self.feature_matching_loss(real_feats, fake_feats)\n",
    "        gen_loss = self.generator_loss(fake_outputs)\n",
    "        disc_loss = self.discriminator_loss(real_outputs, fake_outputs)\n",
    "        \n",
    "        # Total losses\n",
    "        g_loss = mel_loss + fm_loss + gen_loss\n",
    "        d_loss = disc_loss\n",
    "        \n",
    "        return {\n",
    "            'generator_loss': g_loss,\n",
    "            'discriminator_loss': d_loss,\n",
    "            'mel_loss': mel_loss,\n",
    "            'feature_matching_loss': fm_loss,\n",
    "            'adversarial_loss_g': gen_loss,\n",
    "            'adversarial_loss_d': disc_loss\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mask_from_lengths(lengths):\n",
    "        max_len = torch.max(lengths).item()\n",
    "        ids = torch.arange(0, max_len, device=lengths.device)\n",
    "        mask = (ids < lengths.unsqueeze(1)).bool()\n",
    "        return mask"
   ],
   "id": "59dde7bd446e9993",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:15.423666Z",
     "start_time": "2024-11-26T06:37:15.416985Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class LocationLayer(nn.Module):\n",
    "    def __init__(self, attention_n_filters, attention_kernel_size, attention_dim):\n",
    "        super().__init__()\n",
    "        padding = int((attention_kernel_size - 1) / 2)\n",
    "        self.location_conv = nn.Conv1d(2, attention_n_filters, \n",
    "                                     kernel_size=attention_kernel_size,\n",
    "                                     padding=padding, bias=False)\n",
    "        self.location_dense = nn.Linear(attention_n_filters, attention_dim, bias=False)\n",
    "\n",
    "    def forward(self, attention_weights_cat):\n",
    "        processed_attention = self.location_conv(attention_weights_cat)\n",
    "        processed_attention = processed_attention.transpose(1, 2)\n",
    "        processed_attention = self.location_dense(processed_attention)\n",
    "        return processed_attention"
   ],
   "id": "c61cd5751280f699",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:15.602929Z",
     "start_time": "2024-11-26T06:37:15.595097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, attention_rnn_dim, embedding_dim, attention_dim,\n",
    "                 attention_location_n_filters, attention_location_kernel_size):\n",
    "        super().__init__()\n",
    "        self.query_layer = nn.Linear(attention_rnn_dim, attention_dim, bias=False)\n",
    "        self.memory_layer = nn.Linear(embedding_dim, attention_dim, bias=False)\n",
    "        self.v = nn.Linear(attention_dim, 1, bias=False)\n",
    "        self.location_layer = LocationLayer(attention_location_n_filters,\n",
    "                                          attention_location_kernel_size,\n",
    "                                          attention_dim)\n",
    "        self.score_mask_value = -float(\"inf\")\n",
    "\n",
    "    def get_alignment_energies(self, query, processed_memory,\n",
    "                             attention_weights_cat):\n",
    "        processed_query = self.query_layer(query.unsqueeze(1))\n",
    "        processed_attention_weights = self.location_layer(attention_weights_cat)\n",
    "        energies = self.v(torch.tanh(\n",
    "            processed_query + processed_memory + processed_attention_weights))\n",
    "        return energies.squeeze(-1)\n",
    "\n",
    "    def forward(self, attention_hidden_state, memory, processed_memory,\n",
    "                attention_weights_cat, mask):\n",
    "        alignment = self.get_alignment_energies(\n",
    "            attention_hidden_state, processed_memory, attention_weights_cat)\n",
    "\n",
    "        if mask is not None:\n",
    "            alignment.data.masked_fill_(mask, self.score_mask_value)\n",
    "\n",
    "        attention_weights = F.softmax(alignment, dim=1)\n",
    "        attention_context = torch.bmm(attention_weights.unsqueeze(1), memory)\n",
    "        attention_context = attention_context.squeeze(1)\n",
    "\n",
    "        return attention_context, attention_weights\n"
   ],
   "id": "174f59edc6d1431",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:16.196673Z",
     "start_time": "2024-11-26T06:37:16.189445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Prenet(nn.Module):\n",
    "    def __init__(self, in_dim, sizes):\n",
    "        super().__init__()\n",
    "        in_sizes = [in_dim] + sizes[:-1]\n",
    "        self.layers = nn.ModuleList(\n",
    "            [nn.Linear(in_size, out_size)\n",
    "             for (in_size, out_size) in zip(in_sizes, sizes)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for linear in self.layers:\n",
    "            x = F.dropout(F.relu(linear(x)), p=0.5, training=True)\n",
    "        return x"
   ],
   "id": "a37d55d6b2b876dc",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:16.739077Z",
     "start_time": "2024-11-26T06:37:16.729199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Postnet(nn.Module):\n",
    "    def __init__(self, n_mel_channels, postnet_embedding_dim,\n",
    "                 postnet_kernel_size, postnet_n_convolutions):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.convolutions = nn.ModuleList()\n",
    "        \n",
    "        # 첫 번째 컨볼루션 레이어\n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(n_mel_channels, postnet_embedding_dim,\n",
    "                         kernel_size=postnet_kernel_size, stride=1,\n",
    "                         padding=(postnet_kernel_size - 1) // 2),\n",
    "                nn.BatchNorm1d(postnet_embedding_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Dropout(0.5)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 중간 컨볼루션 레이어들\n",
    "        for _ in range(1, postnet_n_convolutions - 1):\n",
    "            self.convolutions.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(postnet_embedding_dim, postnet_embedding_dim,\n",
    "                             kernel_size=postnet_kernel_size, stride=1,\n",
    "                             padding=(postnet_kernel_size - 1) // 2),\n",
    "                    nn.BatchNorm1d(postnet_embedding_dim),\n",
    "                    nn.Tanh(),\n",
    "                    nn.Dropout(0.5)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # 마지막 컨볼루션 레이어\n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(postnet_embedding_dim, n_mel_channels,\n",
    "                         kernel_size=postnet_kernel_size, stride=1,\n",
    "                         padding=(postnet_kernel_size - 1) // 2),\n",
    "                nn.BatchNorm1d(n_mel_channels),\n",
    "                nn.Dropout(0.5)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: 멜 스펙트로그램 [batch_size, n_mel_channels, time]\n",
    "        Returns:\n",
    "            수정된 멜 스펙트로그램\n",
    "        \"\"\"\n",
    "        for i in range(len(self.convolutions) - 1):\n",
    "            x = self.convolutions[i](x)\n",
    "        x = self.convolutions[-1](x)\n",
    "        return x"
   ],
   "id": "fd3b7defe2ec4875",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:17.449719Z",
     "start_time": "2024-11-26T06:37:17.436813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder_embedding_dim, encoder_n_convolutions,\n",
    "                 encoder_kernel_size, encoder_lstm_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        convolutions = []\n",
    "        for _ in range(encoder_n_convolutions):\n",
    "            conv_layer = nn.Sequential(\n",
    "                nn.Conv1d(encoder_embedding_dim, encoder_embedding_dim,\n",
    "                         encoder_kernel_size, stride=1,\n",
    "                         padding=int((encoder_kernel_size - 1) / 2)),\n",
    "                nn.BatchNorm1d(encoder_embedding_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.5)\n",
    "            )\n",
    "            convolutions.append(conv_layer)\n",
    "        self.convolutions = nn.ModuleList(convolutions)\n",
    "        \n",
    "        self.lstm = nn.LSTM(encoder_embedding_dim, encoder_lstm_dim,\n",
    "                           num_layers=1, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x, input_lengths):\n",
    "        \"\"\"\n",
    "        x: [B, embed_dim, T]\n",
    "        input_lengths: [B]\n",
    "        \"\"\"\n",
    "        # Conv layers\n",
    "        for conv in self.convolutions:\n",
    "            x = conv(x)\n",
    "        \n",
    "        # Prepare for LSTM\n",
    "        x = x.transpose(1, 2)  # [B, T, embed_dim]\n",
    "        \n",
    "        # Pack sequence\n",
    "        input_lengths = input_lengths.cpu()  # lengths를 CPU로 이동\n",
    "        \n",
    "        # Sort by length for packing\n",
    "        input_lengths, sort_idx = torch.sort(input_lengths, descending=True)\n",
    "        x = x[sort_idx]\n",
    "        \n",
    "        # Pack the sequence\n",
    "        x_packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            x, input_lengths.cpu(), batch_first=True)\n",
    "        \n",
    "        # LSTM forward\n",
    "        self.lstm.flatten_parameters()\n",
    "        outputs, _ = self.lstm(x_packed)\n",
    "        \n",
    "        # Unpack the sequence\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(\n",
    "            outputs, batch_first=True)\n",
    "        \n",
    "        # Restore original order\n",
    "        _, unsort_idx = torch.sort(sort_idx)\n",
    "        outputs = outputs[unsort_idx]\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def inference(self, x):\n",
    "        for conv in self.convolutions:\n",
    "            x = conv(x)\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        self.lstm.flatten_parameters()\n",
    "        outputs, _ = self.lstm(x)\n",
    "\n",
    "        return outputs"
   ],
   "id": "fbdb327638f5d2e3",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:17.965286Z",
     "start_time": "2024-11-26T06:37:17.942282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_mel_channels, encoder_embedding_dim,\n",
    "                 attention_dim, attention_location_n_filters,\n",
    "                 attention_location_kernel_size, attention_rnn_dim,\n",
    "                 decoder_rnn_dim, prenet_dim, max_decoder_steps,\n",
    "                 gate_threshold, p_attention_dropout, p_decoder_dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_mel_channels = n_mel_channels\n",
    "        self.encoder_embedding_dim = encoder_embedding_dim\n",
    "        self.attention_rnn_dim = attention_rnn_dim\n",
    "        self.decoder_rnn_dim = decoder_rnn_dim\n",
    "        self.prenet_dim = prenet_dim\n",
    "        self.max_decoder_steps = max_decoder_steps\n",
    "        self.gate_threshold = gate_threshold\n",
    "        self.p_attention_dropout = p_attention_dropout\n",
    "        self.p_decoder_dropout = p_decoder_dropout\n",
    "\n",
    "        # Prenet\n",
    "        self.prenet = Prenet(n_mel_channels, [prenet_dim, prenet_dim])\n",
    "\n",
    "        # Attention RNN\n",
    "        self.attention_rnn = nn.LSTMCell(\n",
    "            prenet_dim + encoder_embedding_dim,\n",
    "            attention_rnn_dim)\n",
    "\n",
    "        # Attention Layer\n",
    "        self.attention_layer = Attention(\n",
    "            attention_rnn_dim, encoder_embedding_dim,\n",
    "            attention_dim, attention_location_n_filters,\n",
    "            attention_location_kernel_size)\n",
    "\n",
    "        # Decoder RNN\n",
    "        self.decoder_rnn = nn.LSTMCell(\n",
    "            attention_rnn_dim + encoder_embedding_dim,\n",
    "            decoder_rnn_dim)\n",
    "\n",
    "        # Linear Projection\n",
    "        self.linear_projection = nn.Linear(\n",
    "            decoder_rnn_dim + encoder_embedding_dim,\n",
    "            n_mel_channels)  # 출력을 n_mel_channels로 수정\n",
    "\n",
    "        # Gate Layer\n",
    "        self.gate_layer = nn.Linear(\n",
    "            decoder_rnn_dim + encoder_embedding_dim, 1,\n",
    "            bias=True)\n",
    "\n",
    "        # Attention 관련 레이어 추가\n",
    "        self.attention_layer = Attention(\n",
    "            attention_rnn_dim,\n",
    "            encoder_embedding_dim,\n",
    "            attention_dim,\n",
    "            attention_location_n_filters,\n",
    "            attention_location_kernel_size\n",
    "        )\n",
    "        \n",
    "        # Memory layer 추가\n",
    "        self.memory_layer = nn.Linear(\n",
    "            encoder_embedding_dim,\n",
    "            attention_dim,\n",
    "            bias=False\n",
    "        )\n",
    "        \n",
    "        # Attention context projection\n",
    "        self.attention_projection = nn.Linear(\n",
    "            encoder_embedding_dim,\n",
    "            decoder_rnn_dim,\n",
    "            bias=False\n",
    "        )\n",
    "\n",
    "    def parse_decoder_inputs(self, decoder_inputs):\n",
    "        \"\"\" Prepares decoder inputs, i.e. mel outputs\n",
    "        Args:\n",
    "            decoder_inputs: inputs used for teacher-forced training, i.e. mel-specs\n",
    "        \"\"\"\n",
    "        # (B, n_mel_channels, T) -> (B, T, n_mel_channels)\n",
    "        decoder_inputs = decoder_inputs.transpose(1, 2)\n",
    "        decoder_inputs = decoder_inputs.contiguous()\n",
    "        \n",
    "        # (B, T, n_mel_channels) -> (T, B, n_mel_channels)\n",
    "        decoder_inputs = decoder_inputs.transpose(0, 1)\n",
    "        return decoder_inputs\n",
    "\n",
    "    def parse_decoder_outputs(self, mel_outputs, gate_outputs, alignments):\n",
    "        \"\"\" Prepares decoder outputs for output\n",
    "        Args:\n",
    "            mel_outputs: mel outputs from the decoder\n",
    "            gate_outputs: gate outputs from the decoder\n",
    "            alignments: alignments from the decoder\n",
    "        \"\"\"\n",
    "        # (T, B, n_mel_channels) -> (B, T, n_mel_channels)\n",
    "        mel_outputs = mel_outputs.transpose(0, 1).contiguous()\n",
    "        \n",
    "        # (T, B) -> (B, T)\n",
    "        gate_outputs = gate_outputs.transpose(0, 1).contiguous()\n",
    "        \n",
    "        # (T, B, n_text) -> (B, T, n_text)\n",
    "        alignments = alignments.transpose(0, 1).contiguous()\n",
    "\n",
    "        return mel_outputs, gate_outputs, alignments\n",
    "\n",
    "    def forward(self, encoder_outputs, decoder_inputs, memory_lengths=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            encoder_outputs: [batch_size, max_time, encoder_embedding_dim]\n",
    "            decoder_inputs: [batch_size, n_mel_channels, max_time]\n",
    "            memory_lengths: [batch_size]\n",
    "        \"\"\"\n",
    "        # decoder_inputs 형태 변환\n",
    "        decoder_inputs = decoder_inputs.transpose(1, 2)  # [batch_size, max_time, n_mel_channels]\n",
    "        \n",
    "        # 초기 상태 초기화\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        max_time = decoder_inputs.size(1)\n",
    "        \n",
    "        # 초기 attention context\n",
    "        attention_context = torch.zeros(\n",
    "            batch_size,\n",
    "            self.encoder_embedding_dim\n",
    "        ).to(encoder_outputs.device)\n",
    "        \n",
    "        # 초기 attention hidden states\n",
    "        attention_hidden = torch.zeros(\n",
    "            batch_size,\n",
    "            self.attention_rnn_dim\n",
    "        ).to(encoder_outputs.device)\n",
    "        \n",
    "        attention_cell = torch.zeros(\n",
    "            batch_size,\n",
    "            self.attention_rnn_dim\n",
    "        ).to(encoder_outputs.device)\n",
    "        \n",
    "        # 초기 decoder states\n",
    "        decoder_hidden = torch.zeros(\n",
    "            batch_size,\n",
    "            self.decoder_rnn_dim\n",
    "        ).to(encoder_outputs.device)\n",
    "        \n",
    "        decoder_cell = torch.zeros(\n",
    "            batch_size,\n",
    "            self.decoder_rnn_dim\n",
    "        ).to(encoder_outputs.device)\n",
    "        \n",
    "        # 초기 attention weights\n",
    "        attention_weights = torch.zeros(\n",
    "            batch_size,\n",
    "            encoder_outputs.size(1)\n",
    "        ).to(encoder_outputs.device)\n",
    "        \n",
    "        # 출력을 저장할 리스트\n",
    "        mel_outputs, gate_outputs, alignments = [], [], []\n",
    "        \n",
    "        # Memory를 미리 처리\n",
    "        processed_memory = self.memory_layer(encoder_outputs)\n",
    "        \n",
    "        # 각 타임스텝에 대해 처리\n",
    "        for i in range(max_time):\n",
    "            current_input = decoder_inputs[:, i, :]  # [batch_size, n_mel_channels]\n",
    "            current_input = self.prenet(current_input)  # [batch_size, prenet_dim]\n",
    "            \n",
    "            # Attention RNN\n",
    "            cell_input = torch.cat((current_input, attention_context), -1)\n",
    "            attention_hidden, attention_cell = self.attention_rnn(\n",
    "                cell_input, (attention_hidden, attention_cell))\n",
    "            attention_hidden = F.dropout(\n",
    "                attention_hidden, self.p_attention_dropout, self.training)\n",
    "            \n",
    "            # Attention 계산\n",
    "            attention_weights_cat = torch.cat(\n",
    "                (attention_weights.unsqueeze(1),\n",
    "                 attention_weights.unsqueeze(1)),\n",
    "                dim=1)\n",
    "            attention_context, attention_weights = self.attention_layer(\n",
    "                attention_hidden, encoder_outputs,\n",
    "                processed_memory, attention_weights_cat,\n",
    "                mask=None if memory_lengths is None else ~get_mask_from_lengths(memory_lengths))\n",
    "            \n",
    "            # Decoder RNN\n",
    "            decoder_input = torch.cat((attention_hidden, attention_context), -1)\n",
    "            decoder_hidden, decoder_cell = self.decoder_rnn(\n",
    "                decoder_input, (decoder_hidden, decoder_cell))\n",
    "            decoder_hidden = F.dropout(\n",
    "                decoder_hidden, self.p_decoder_dropout, self.training)\n",
    "            \n",
    "            # Linear projection\n",
    "            decoder_hidden_attention = torch.cat(\n",
    "                (decoder_hidden, attention_context), dim=1)\n",
    "            decoder_output = self.linear_projection(decoder_hidden_attention)\n",
    "            gate_prediction = self.gate_layer(decoder_hidden_attention)\n",
    "            \n",
    "            # 결과 저장\n",
    "            mel_outputs.append(decoder_output)\n",
    "            gate_outputs.append(gate_prediction)\n",
    "            alignments.append(attention_weights)\n",
    "        \n",
    "        # 리스트를 텐서로 변환\n",
    "        mel_outputs = torch.stack(mel_outputs)\n",
    "        gate_outputs = torch.stack(gate_outputs)\n",
    "        alignments = torch.stack(alignments)\n",
    "        \n",
    "        # 출력 형식 변환\n",
    "        mel_outputs, gate_outputs, alignments = self.parse_decoder_outputs(\n",
    "            mel_outputs, gate_outputs, alignments)\n",
    "        \n",
    "        return mel_outputs, gate_outputs, alignments\n"
   ],
   "id": "81e8237c2b5799d6",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:19.048799Z",
     "start_time": "2024-11-26T06:37:19.032971Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Tacotron2(nn.Module):\n",
    "    attention_location_n_filters = 32\n",
    "    attention_location_kernel_size = 31\n",
    "    decoder_rnn_dim = 512\n",
    "    prenet_dim = 256\n",
    "    max_decoder_steps = 1000\n",
    "    gate_threshold = 0.5\n",
    "    p_attention_dropout = 0.1\n",
    "    p_decoder_dropout = 0.1\n",
    "    postnet_embedding_dim = 256\n",
    "    postnet_kernel_size = 5\n",
    "\n",
    "    def __init__(self, n_mel_channels, vocab_size, embedding_dim, \n",
    "                 encoder_n_convolutions, encoder_kernel_size,\n",
    "                 attention_rnn_dim, attention_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.encoder = Encoder(\n",
    "            encoder_embedding_dim=embedding_dim,  # 256\n",
    "            encoder_n_convolutions=encoder_n_convolutions,\n",
    "            encoder_kernel_size=encoder_kernel_size,\n",
    "            encoder_lstm_dim=int(embedding_dim/2)  # 128 (bi-directional = 256)\n",
    "        )\n",
    "        \n",
    "        self.decoder = Decoder(\n",
    "            n_mel_channels=n_mel_channels,\n",
    "            encoder_embedding_dim=embedding_dim,  # 256\n",
    "            attention_dim=attention_dim,  # 128\n",
    "            attention_location_n_filters=self.attention_location_n_filters,\n",
    "            attention_location_kernel_size=self.attention_location_kernel_size,\n",
    "            attention_rnn_dim=attention_rnn_dim,  # 512\n",
    "            decoder_rnn_dim=self.decoder_rnn_dim,  # 512\n",
    "            prenet_dim=self.prenet_dim,  # 128\n",
    "            max_decoder_steps=self.max_decoder_steps,\n",
    "            gate_threshold=self.gate_threshold,\n",
    "            p_attention_dropout=self.p_attention_dropout,\n",
    "            p_decoder_dropout=self.p_decoder_dropout\n",
    "        )\n",
    "        \n",
    "        self.postnet = Postnet(\n",
    "            n_mel_channels=n_mel_channels,\n",
    "            postnet_embedding_dim=self.postnet_embedding_dim,\n",
    "            postnet_kernel_size=self.postnet_kernel_size,\n",
    "            postnet_n_convolutions=5\n",
    "        )\n",
    "\n",
    "    def forward(self, text_inputs, text_lengths, mel_inputs, mel_lengths):\n",
    "        \"\"\"\n",
    "        텍스트를 멜 스펙트로그램으로 변환\n",
    "        \"\"\"\n",
    "        embedded_inputs = self.embedding(text_inputs).transpose(1, 2)\n",
    "        \n",
    "        encoder_outputs = self.encoder(embedded_inputs, text_lengths)\n",
    "        \n",
    "        mel_outputs, gate_outputs, alignments = self.decoder(\n",
    "            encoder_outputs, mel_inputs, text_lengths)\n",
    "        \n",
    "        # mel_outputs 차원 변환 (B, T, n_mel_channels) -> (B, n_mel_channels, T)\n",
    "        mel_outputs_transpose = mel_outputs.transpose(1, 2)\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs_transpose)\n",
    "        mel_outputs_postnet = mel_outputs_transpose + mel_outputs_postnet\n",
    "        \n",
    "        # 결과를 다시 원래 형태로 변환 (B, n_mel_channels, T) -> (B, T, n_mel_channels)\n",
    "        mel_outputs_postnet = mel_outputs_postnet.transpose(1, 2)\n",
    "        \n",
    "        return mel_outputs_postnet, mel_outputs, gate_outputs, alignments\n",
    "\n",
    "    def inference(self, text_inputs):\n",
    "        \"\"\"\n",
    "        추론 시 사용되는 forward 메서드\n",
    "        \"\"\"\n",
    "        embedded_inputs = self.embedding(text_inputs).transpose(1, 2)\n",
    "        encoder_outputs = self.encoder.inference(embedded_inputs)\n",
    "        mel_outputs, gate_outputs, alignments = self.decoder.inference(encoder_outputs)\n",
    "        mel_outputs_postnet = self.postnet(mel_outputs)\n",
    "        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n",
    "        \n",
    "        return mel_outputs_postnet, mel_outputs, gate_outputs, alignments\n"
   ],
   "id": "455fb4c9d54c8d4d",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:37:20.988356Z",
     "start_time": "2024-11-26T06:37:20.981386Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_mask_from_lengths(lengths):\n",
    "    max_len = torch.max(lengths).item()\n",
    "    ids = torch.arange(0, max_len, device=lengths.device)\n",
    "    mask = (ids < lengths.unsqueeze(1)).bool()\n",
    "    return mask "
   ],
   "id": "77f284c8bd764f75",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:38:32.569774Z",
     "start_time": "2024-11-26T06:38:32.563410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from argparse import Namespace\n",
    "    \n",
    "config = Namespace(\n",
    "    device_num=0,\n",
    "    batch_size=4,\n",
    "    learning_rate=0.0002,\n",
    "    num_epochs=100,\n",
    "    start_epoch=0,\n",
    "    save_interval=10,\n",
    "    checkpoint_dir='checkpoints',\n",
    "    resume_checkpoint=None\n",
    ")"
   ],
   "id": "85ca8a468aa3bedb",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T06:41:44.546176Z",
     "start_time": "2024-11-26T06:38:33.377727Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer = Trainer(config)\n",
    "trainer.train()     "
   ],
   "id": "e389a0f54f7d3f46",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "README.md:   0%|          | 0.00/4.45k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b7c9405712834ef69d45718d35b7347e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\com\\PycharmProjects\\CLIP\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\com\\.cache\\huggingface\\hub\\datasets--Bingsu--KSS_Dataset. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/1.09k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "005e10ad96ee43e5b568b48dfad7000e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train-00000-of-00007.parquet:   0%|          | 0.00/556M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0251cdc9228d4c1ba87e7492aa77c534"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train-00001-of-00007.parquet:   0%|          | 0.00/600M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e62dbebb49ad4f319f286cbb9a0f0845"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train-00002-of-00007.parquet:   0%|          | 0.00/586M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e64721751b8a4f6f85ce253988d6ad04"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train-00003-of-00007.parquet:   0%|          | 0.00/497M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7962d727506b41a1b871d3117935c860"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train-00004-of-00007.parquet:   0%|          | 0.00/495M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5cc69d4b1531414e83c597fdbf35abd9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train-00005-of-00007.parquet:   0%|          | 0.00/530M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "817a90fe5418440aa4fa83c2262c66e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train-00006-of-00007.parquet:   0%|          | 0.00/544M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "202ae13a184845e99cd7a1e8869bcc9f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split:   0%|          | 0/12854 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a563e066bbee483c82f8a0176d2178f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "File \u001B[1;32m~\\PycharmProjects\\CLIP\\.venv\\Lib\\site-packages\\datasets\\builder.py:1870\u001B[0m, in \u001B[0;36mArrowBasedBuilder._prepare_split_single\u001B[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001B[0m\n\u001B[0;32m   1869\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 1870\u001B[0m     \u001B[43mwriter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtable\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1871\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m CastError \u001B[38;5;28;01mas\u001B[39;00m cast_error:\n",
      "File \u001B[1;32m~\\PycharmProjects\\CLIP\\.venv\\Lib\\site-packages\\datasets\\arrow_writer.py:627\u001B[0m, in \u001B[0;36mArrowWriter.write_table\u001B[1;34m(self, pa_table, writer_batch_size)\u001B[0m\n\u001B[0;32m    626\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_examples \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m pa_table\u001B[38;5;241m.\u001B[39mnum_rows\n\u001B[1;32m--> 627\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpa_writer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpa_table\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwriter_batch_size\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\CLIP\\.venv\\Lib\\site-packages\\pyarrow\\ipc.pxi:529\u001B[0m, in \u001B[0;36mpyarrow.lib._CRecordBatchWriter.write_table\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\CLIP\\.venv\\Lib\\site-packages\\pyarrow\\error.pxi:89\u001B[0m, in \u001B[0;36mpyarrow.lib.check_status\u001B[1;34m()\u001B[0m\n",
      "File \u001B[1;32m~\\PycharmProjects\\CLIP\\.venv\\Lib\\site-packages\\fsspec\\implementations\\local.py:373\u001B[0m, in \u001B[0;36mLocalFileOpener.write\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m    372\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrite\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 373\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mOSError\u001B[0m: [Errno 28] No space left on device",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mDatasetGenerationError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[35], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      2\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain() \n",
      "Cell \u001B[1;32mIn[16], line 11\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[1;34m(self, config)\u001B[0m\n\u001B[0;32m      8\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# 데이터셋 및 데이터로더 초기화\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mKSSTTSDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvalid_dataset \u001B[38;5;241m=\u001B[39m KSSTTSDataset(split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalid\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_loader \u001B[38;5;241m=\u001B[39m DataLoader(\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_dataset,\n\u001B[0;32m     16\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39mconfig\u001B[38;5;241m.\u001B[39mbatch_size,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     19\u001B[0m     num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m\n\u001B[0;32m     20\u001B[0m )\n",
      "Cell \u001B[1;32mIn[17], line 10\u001B[0m, in \u001B[0;36mKSSTTSDataset.__init__\u001B[1;34m(self, split, valid_size, seed, target_sr)\u001B[0m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;66;03m# 데이터셋 로드\u001B[39;00m\n\u001B[1;32m---> 10\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mBingsu/KSS_Dataset\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     11\u001B[0m full_dataset \u001B[38;5;241m=\u001B[39m dataset[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     13\u001B[0m \u001B[38;5;66;03m# train/valid 분할\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\CLIP\\.venv\\Lib\\site-packages\\datasets\\load.py:2154\u001B[0m, in \u001B[0;36mload_dataset\u001B[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[0;32m   2151\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m builder_instance\u001B[38;5;241m.\u001B[39mas_streaming_dataset(split\u001B[38;5;241m=\u001B[39msplit)\n\u001B[0;32m   2153\u001B[0m \u001B[38;5;66;03m# Download and prepare data\u001B[39;00m\n\u001B[1;32m-> 2154\u001B[0m \u001B[43mbuilder_instance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2155\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2157\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverification_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverification_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2158\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2159\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   2160\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2162\u001B[0m \u001B[38;5;66;03m# Build dataset for splits\u001B[39;00m\n\u001B[0;32m   2163\u001B[0m keep_in_memory \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m   2164\u001B[0m     keep_in_memory \u001B[38;5;28;01mif\u001B[39;00m keep_in_memory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m is_small_dataset(builder_instance\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size)\n\u001B[0;32m   2165\u001B[0m )\n",
      "File \u001B[1;32m~\\PycharmProjects\\CLIP\\.venv\\Lib\\site-packages\\datasets\\builder.py:924\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[1;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001B[0m\n\u001B[0;32m    922\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_proc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    923\u001B[0m     prepare_split_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_proc\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_proc\n\u001B[1;32m--> 924\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    925\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdl_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdl_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    926\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverification_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverification_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    927\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mprepare_split_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    928\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdownload_and_prepare_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    929\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    930\u001B[0m \u001B[38;5;66;03m# Sync info\u001B[39;00m\n\u001B[0;32m    931\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(split\u001B[38;5;241m.\u001B[39mnum_bytes \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39msplits\u001B[38;5;241m.\u001B[39mvalues())\n",
      "File \u001B[1;32m~\\PycharmProjects\\CLIP\\.venv\\Lib\\site-packages\\datasets\\builder.py:1000\u001B[0m, in \u001B[0;36mDatasetBuilder._download_and_prepare\u001B[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001B[0m\n\u001B[0;32m    996\u001B[0m split_dict\u001B[38;5;241m.\u001B[39madd(split_generator\u001B[38;5;241m.\u001B[39msplit_info)\n\u001B[0;32m    998\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    999\u001B[0m     \u001B[38;5;66;03m# Prepare split will record examples associated to the split\u001B[39;00m\n\u001B[1;32m-> 1000\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prepare_split\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_generator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mprepare_split_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1001\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1002\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mOSError\u001B[39;00m(\n\u001B[0;32m   1003\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot find data file. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1004\u001B[0m         \u001B[38;5;241m+\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_download_instructions \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m   1005\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mOriginal error:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1006\u001B[0m         \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mstr\u001B[39m(e)\n\u001B[0;32m   1007\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\CLIP\\.venv\\Lib\\site-packages\\datasets\\builder.py:1741\u001B[0m, in \u001B[0;36mArrowBasedBuilder._prepare_split\u001B[1;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001B[0m\n\u001B[0;32m   1739\u001B[0m job_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m   1740\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m pbar:\n\u001B[1;32m-> 1741\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mjob_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_prepare_split_single\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1742\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgen_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgen_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjob_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mjob_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_prepare_split_args\u001B[49m\n\u001B[0;32m   1743\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m   1744\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdone\u001B[49m\u001B[43m:\u001B[49m\n\u001B[0;32m   1745\u001B[0m \u001B[43m            \u001B[49m\u001B[43mresult\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mcontent\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\CLIP\\.venv\\Lib\\site-packages\\datasets\\builder.py:1897\u001B[0m, in \u001B[0;36mArrowBasedBuilder._prepare_split_single\u001B[1;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001B[0m\n\u001B[0;32m   1895\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(e, DatasetGenerationError):\n\u001B[0;32m   1896\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m-> 1897\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DatasetGenerationError(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while generating the dataset\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[0;32m   1899\u001B[0m \u001B[38;5;28;01myield\u001B[39;00m job_id, \u001B[38;5;28;01mTrue\u001B[39;00m, (total_num_examples, total_num_bytes, writer\u001B[38;5;241m.\u001B[39m_features, num_shards, shard_lengths)\n",
      "\u001B[1;31mDatasetGenerationError\u001B[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7aa54c83868a5e2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
